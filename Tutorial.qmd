---
title: "Tutorial"
format: html
bibliography: bibliography/Boot-Err_Tutorial.bib
csl: bibliography/apa-6th-edition.csl
---

# New

Intro - initial question / that is best answered using Boot-Err

Notes Brainstorming Jens:

-   was kann RGMA nicht? Prädiktoren hinzufügen!

    -   Shaw etc! Prädiktoren

Perspektive:

Messinstrument?

Standardisierte Effekte?

-   Forschungsfrage pro Methode vs. unterschiedliche Definition pro Methode

-   Wenn Rel immer gleich, aber varianzen unterschiedlich, dann muss das Heteroskedastizität implizieren?

-   partial pooling um schätzungen der "wahren" Fehlervarianzen zu generieren

Jens Fünderich, 2026: Punktschätzungen der wahren Fehlervarianz

## Introduction

Researchers frequently administer the same psychological scale across different samples, settings or populations. A scale may be translated into another language, administered online instead of in person, or completed by participants with different educational or cultural backgrounds. In such situations, an important question arises: does the instrument produce scores of equivalent measurement quality across these administrations?

At first glance, this question appears straightforward. However, what is meant by measurement quality is less obvious. Throughout this tutorial, we focus on random measurement error as a central component of measurement quality. Random measurement error refers to unsystematic fluctuations in observed scores around the values they would take on in the absence of measurement imprecision. Unlike systematic error, which shifts observed scores in a particular direction, random error introduces general fuzziness, often described as noise. On average, observed scores may still be accurate, but individual measurements vary unpredictably around their underlying true values.

If the same instrument is administered in two different contexts, differences in random measurement error imply differences in measurement quality. However, there are several ways to conceptualize and quantify such differences. Importantly, these approaches do not contradict each other. Rather, they reflect different perspectives on what it means for measurement quality to be equivalent.

The first, most intuitive and widely used answer is to compare reliability coefficients. Reliability Generalization Meta-Analysis extends this idea by formally testing whether oberved differences in score reliability exceed what would be expected by sampling error. A second, more structurally explicit answer is provided by measurement invariance testing. This framework models the relation between latent factors and observed item responses and evaluates whether residual variances (variance not explained by the underlying factor) are equivalent across groups. Finally, Boot-Err introduces a third perspective. It conceptualises measurement quality directly as error score variance and focuses on estimating and comparing this variance component across administrations. While all three approaches address the question of whether measurement quality differs, they do so from conceptually distinct perspectives and therefore lead to different interpretations.

Throughout this tutorial, we will explore the perspectives and interpretations from the three approaches using an explicit example. Consider a scale measuring Extraversion administered to a wide range of different samples, under which we find a German sample of Psychology students and an international online panel for market research. The concept of Extraversion is thought to be generalizable across different populations and potentially linked to a larger number of different effects and phenomena. To explore these links, however, we need to make sure that we are measuring comparably well across all administration sites - otherwise we cannot explore to what extent Extraversion actually generalizes across contexts and to what extent the links to other effects and phenomena hold up across contexts.

### Reliability generalization meta-analysis

A simple and accessible way to compare measuring quality across administrations of the same scale is by comparing the reliability coefficients computed for each administration sample. The conceptual idea behind reliability coefficients can be described in terms of classical test theory. Classical test theory implies that each score $X$ is made up of true score $T$ and error score $E$:

$$ X = T + E$$ {#eq-CTT}

Reliability coefficients, such as Cronbach's Alpha, McDonald's Omega or Guttmann's Lambda IV attempt to quantify the proportion of true score variance present in the scores:

$$\rho_{XX'} = \frac{\sigma^2_T}{\sigma^2_X}$$ {#eq-reliability}

where $\rho_{XX'}$ is score reliability, $\sigma^2_T$ is the true score variance and $\sigma^2_X$ is the total score variance, made up of true and error score variance ($\sigma^2_X = \sigma^2_T + \sigma^2_E$, where $\sigma^2_E$ is the error score variance).

This true score variance $\sigma^2_T$ describes the variation in the scores, that can be attributed to genuine differences in the underlying factor being measured. For the purpose of this tutorial, we will refer to it as *signal* - the part of the score that we actually intended to measure. Error score variance $\sigma^2_E$ on the other hand describes the variation in the scores, that can be attributed to random, unsystematic variation - random measurement error that is introduced by the instruments inability to perfectly measure the underlying factor, which we will refer to as *noise* for the purpose of this tutorial. Thereby, these reliability coefficients essentially describe which proportion of the observed score variance can *not* be attributed to noise. Differences in reliability coefficients therefore highlight, that the proportion of signal to total variation in the data differs across administration sites.

Generally, reliability coefficients can take on values between 0 and 1, where 0 indicates that the variance in observations is made up of 100 % noise, while a 1 indicates that there is no noise in the data. Returning to our example, assume we find a Cronbach's alpha .67 for the scores obtained from our sample of Psychology students, but a Cronbach's alpha of .75 for observations stemming from our online panel. Comparing these numbers, we can conclude that the proportion of signal to total variation in the scores is worse for data obtained from Psychology students than for data obtained from the online panel. Of the total score variance observed in each individual sample, a higher proportion can be attributed to true differences in Extraversion for the online panel, than for the student sample.

However, coefficients derived from samples are always prone to sampling error. We are aware of this truth everytime we make use of a test for statistical significance of a mean value or standardised effect size (or estimate a posterior distribution for mean value / effect size). Just like the population mean value is not necessarily identical to the estimate obtained from the sample, the same is true for the estimated reliability coefficient. Due to the randomness in the sampling process, the estimated mean value or reliability coefficient may vary randomly around the population value.

This means that we can not simply compare the estimates of reliability coefficients to determine whether there are true differences in measurement precision across administration sites. However, a formal method to assess whether reliability coefficients of the same instrument differ across administrations has been developed in the form of reliability generalization meta-analysis. Comparing and combining estimates from several studies or samples to derive appropriate mean values and compute how strongly the population values underlying these estimates diverge is known as research synthesis. Techniques in the space of research synthesis typically attempt to model the sampling error introduced by the randomness of the sampling procedure, and correct for it when testing statistical significance or generating estimates. Meta-analysis is a technique that incorporates the uncertainty of sample estimates and allows to test for and model the variance that goes beyond the sampling error. This variance, referred to as heterogeneity, describes the variation of the coefficient that we would have observed, if there was no sampling error. If significant heterogeneity is detected, this indicates that reliability differs across adminstrations beyond what would be expected from sampling fluctuations alone. However, it does not identify the source of these differences.

Reliability Generalization Meta-Analysis aggregates estimates of the same reliability coefficient such as Cronbach's Alpha across several administrations of the same scale, to test whether there are statistically significant differences between these coefficients. Additionally, an estimate of heterogeneity can be derived, which describes how strongly the reliability coefficients vary. In our example case, we can compute Cronbach's Alpha (or other reliability coefficients) for our Psychology student sample, for our market research panel and for the other administrations of the scale that we want to compare. Reliability Generalization Meta-Analysis can inform us whether the differences in Cronbach's Alpha are indeed statistically significant. Thereby, we can properly test whether measurement precision in terms of score reliablity differs across administrations.

Score reliability is the ratio of signal to total score variance, as demonstrated in @eq-reliability. The total score variance is essentially made up of the sum of signal and noise. This means that differences in reliability coefficients can not necessarily be attributed to differences in the noise - this is true irrespective of which reliability coefficient is computed. In our example we assumed that score reliability in the sample of Psychology students was .67, while score reliability in the online market research panel was .75. We initially interpreted this as an indicator of impaired measurement quality for the Psychology students' Extraversion, implying larger noise in that sample, but what if the differences actually lie in the signal? Assume that for both samples actually a noise or error score variance of 1 was underlying. It is reasonable to assume that a sample of German Psychology students is put together more homogeneously than an online panel for market research that is deliberately constructed to represent a more diverse population. Therefore, the signal or true score variance might have actually been 2 for the Psychology student sample and 3 for the market research panel. Such a pattern could explain the differences in score reliability irrespective of differences in noise:

$$\rho_{students} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{2}{2 + 1} = .67 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rho_{panel} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{3}{3 + 1} = .75$$ {#eq-RGMA1}

Alternatively, imagine that the noise or error score variance for the online panel might have been 1.5 instead of 1. In that case, the following pattern would emerge:

$$\rho_{students} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{2}{2 + 1} = .67 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rho_{panel} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{3}{3 + 1.5} = .67$$ {#eq-RGMA2}

In this case, reliability coefficients would be have been identical across both samples, even though noise varied across samples.

Both examples demonstrate that reliability coefficients are affected by differences in signal just as much as on differences in noise. They are a relative coefficient, informing us about the relative amount of signal in the total variation. Only if we have reasons to assume that the signal variance should be identical across administrations, then we can interpret results of Reliability Generalization Meta-Analysis as indicative of differences in measurement quality. Consequently, Reliability Generalization Meta-Analysis answers a ratio-based question about measurement quality, not a signal- or noise-based question.

### Measurement invariance testing

An alternative perspective on measurement quality is offered through Measurement Invariance Testing. Measurement Invariance Testing is grounded in Confirmatory Factor Analysis, typically modelled using Structural Equation Modelling. The analyses previously discussed are grounded in classical test theory, implying that each score $X$ is made up of true score $T$ and error score $E$. Measurement Invariance Tests, on the other hand, are based on the modelling of latent variables. Its aim is to model whether the relationship between latent factors and observed item responses is identical across different groups where the same measuring instrument is administered.

Different levels of measurement invariance exist, and in a series of four (sometimes more) steps, different levels are tested. The four levels can be summarised as:

-   Configural invariance: Across all administrations, the measuring instrument assesses the same number of factors. If several factors are assessed, the individual items relate to the same factors across all administrations.

-   Metric invariance: The factor loadings for each individual item are identical across all administrations.

-   Scalar invariance: Systematic differences in the observed scores between administrations can be explained by differences in the underlying factor scores.

-   Residual invariance: The residual variance observed in the item scores, that can not be explained by the underlying factor, is equally large across all administrations.

The last level, the test for residual invariance is of largest interest, when discussing random measurement error or noise in the data. Residual variance describes the variation in scores for a single item (or several), that can not be explained by the latent factor. In the terminology of Classical Test Theory, this can be interpreted as error score variance (albeit there are some caveats, see ref XY). The test for residual invariance assesses whether these variances are identical across all groups. Thereby, Measurement Invariance Testing allows fora formal test, in how far the measuring quality in terms of noise is equivalent across samples.

Returning to our exemplary case, in @eq-RMA2 we observed that identical reliability coefficients might arise, even though measurement quality in terms of noise differed across samples. With some additional assumptions (unidimensionality, tau-equivalence, additive linear model - see reference), that we implicitly assume by averaging item responses to derive individual Extraversion scores, we can use the test for residual invariance to test for such differences in noise. If we observe a pattern like @eq-RMA2, where noise was larger in the market research panel, the test for residual invariance (if power was adequate) should indicate that the test does not display residual invariance across samples. This means the test highlights that there are differences in the noise (here referred to as residual variance). In a pattern like @eq-RMA2, where reliability coefficients differed but noise was identical in market research panel and psychology student sample, the test for residual invariance should demonstrate just that (given adequate power) - there are no differences in noise across administration sites.

The test for residual invariance is essentially tested just like the other levels of measurement invariance testing. A CFA-model with free item residual variances (meaning the item residual variance is estimated from the data) is tested for model fit against a model where the residual variances are constrained to be identical across samples. If the model with free residual variances fits substantially better than the model with constrained residual variances according to a number of test parameters and fit-indices, it indicates that the test does not exhibit residual invariance. Essentially, the hypothesis that the random variation in item scores is identical across all groups has to be rejected.

In practice, researchers evaluate this comparison using both chi-square difference testing and changes in approximate fit indices. A traditional approach is to conduct a chi-square difference test between the unconstrained and constrained models; a statistically significant $\Delta\chi^2\ (p < .05)$ suggests that constraining the residual variances leads to a meaningful loss of model fit, indicating a lack of residual invariance. However, because the chi-square test is highly sensitive to sample size, many researchers additionally rely on changes in practical fit indices.

Commonly examined indices include differences in the Comparative Fit Index ($\Delta CFI$), Root Mean Square Error of Approximation ($\Delta RMSEA$), and Standardized Root Mean Square Residual ($\Delta RSMR$). Small changes in these indices (e.g., $\Delta CFI \leq .1$, $\Delta RMSEA \leq .015$, $\Delta RSMR \leq .01$) are typically interpreted as evidence that the additional constraints do not substantially worsen model fit, thereby supporting residual invariance. By combining statistical significance testing with changes in fit indices, researchers obtain a more robust evaluation of whether residual invariance holds across groups.

If the test for residual invariance indicates that residual invariance *does not hold*, this implies that the residual variance for at least one item differs across groups. In other words, constraining the item-specific error variances to be equal significantly worsens model fit, suggesting that the amount of noise is not identical across groups. When we compute a mean score for Extraversion by averaging the observed item scores, we implicitly assign equal weight to each item. If residual variances differ substantially, this implies that some items contain more measurement error in certain groups, which challenges the assumption of equivalent measurement quality. Thus, testing residual invariance allows us to evaluate whether the amount of noise (i.e., random measurement error) is comparable across groups.

These tests for residual invariance can not inform us on how large the differences in noise (residual variances) are in practice. While it is technically possible to derive estimates regarding the size of residual variance across groups, these tests do not come with an effect size or measure of heterogeneity of how large the differences are across the samples. In our example, we assumed in noise or error score variance where 1 and 1.5 respectively for the online market research panel and the psychology student sample - an absolute difference of .5. However, whether the differences were .5 or 17, the test for residual invariance can not inform us about the magnitude of the difference, just whether there was a difference in the first place.

Generally, tests for measurement invariance come with a number of other limitations, that are not central to this manuscript. Interested readers are referred to publication X & Y.

Measurement invariance testing and the test for residual invariance especially offer a model-based perspective on measurement quality. Essentially, the test for residual invariance allow researchers to test whether measurement quality is equivalent across groups by comparing the item-level residuals. Unlike Reliability Generalization Meta-Analysis, which treated measurement quality as the ratio of signal to total variance, a test for residual invariance can test for absolute differences in noise. The test for residual invariance does not offer a quantification of the differences in said noise. However, this question might be central to the question of differences in measuring quality - maybe we are not only interested in "Are there differences in measuring quality between the scores from the market research panel and the psychology student sample?" but also the question "How large are these differences?". For such questions, Boot-Err might be an appropriate answer, modelling noise explicitly by estimating and testing heterogeneity in error score variance.

### Boot-Err - inspecting differences in random measurement error

Therein, similarly to the test for residual invariance, Boot-Err conceptualises measurement quality in terms of noise. However, in contrast to residual invariance testing, it also provides an estimate of *the magnitude of these differences*. A detailed description on how to perform an analysis using Boot-Err can be found in section "Tutorial". We provide an in-depth description of the analytic formalisation and how it can be derived in Beinhauer et al. [-@beinhauer2025].

To broadly outline the underlying idea: a reliability coefficient quantifies the proportion of true score variance relative to the sample's total score variance. Consequently, error score variance can be derived from the observed score variance and the reliability estimate as

$$\hat{\sigma}^2_E = (1 - \hat{\rho}_{xx'})\ \hat{\sigma}^2_X$${#eq-errorvar}

Boot-Err builds on this identity. Using Bootstrapping techniques, the sampling variability of the estimated error score variance is approximated in terms of the standard error. Appropriate variance-stabilising transformations are applied, so that the error score variance can subsequently be modelled using random-effects meta-analysis (including the appropriate back-transformation). While fixed-effects meta-analyses would also be available, these analyses actually assume that the heterogeneity in the population is zero (ref.). This assumption is loosened in random-effects meta-analyses. As we are actively investigating this heterogeneity, we recommend to make use of analyses where such heterogeneity is actually reflected in the assumptions set. 

These random-effects meta-analyses provide both a test on whether the differences in random error score variance are statistically significant, as well as an estimate of heterogeneity. The heterogeneity quantifies how strongly the error score variance varies across administration settings, with larger values indicating greater variation and therefore larger differences in error score variance.

Unlike analyses of reliability coefficients, error score variance reflects the absolute magnitude of random measurement error in the data rather than its proportion relative to total variance. As a consequence, Boot-Err is not affected by differences in true score variance across samples. Even if the underlying factor is distributed differently across samples, differences random measurement error can still be identified. Similar to measurement equivalence testing, Boot-Err provides a statistical test of whether such differences are present. Beyond this, however, it quantifies their extent in terms of heterogeneity of error score variance across administration settings.

Returning to our example of Extraversion, consider the pattern in @eq-RGMA2, where reliability coefficients did not differ between the market research panel and Psychology student sample. A Reliability Generalization Meta-Analysis would therefore suggest equivalent measurement quality across the two administration sites. A test for residual invariance (assuming appropriate statistical power and no issues in model fitting), would likely indicate that residual variances differ, implying that error score variance differs across the administration settings. However, it would not provide an interpretable estimate of the magnitude of these differences.

Boot-Err would lead to the same substantive conclusion (again, assuming appropriate statistical power). Beyond that, it would additionally yield a quantitative estimate of the heterogeneity in error score variance. For instance, if $\sigma^2_{E,\ students} = 1$ and $\sigma^2_{E,\ panel} = 1.5$, heterogeneity $\tau^2_{\sigma^2_E}$ would be $.065$ ($\tau_{\sigma^2_E} = .25$).

By modelling error score variance directly, Boot-Err enables researchers to quantify differences in measurement quality in terms of random measurement error. Given substantive knowledge about the measuring instrument and the expected magnitude of error score variance, researchers can evaluate whether the differences in measurement quality are negligible or substantively meaningful. If differences in error score variance are small relative to the true score variance $\sigma^2_T$ (or the latent construct's variance), they may have limited practical implications for group comparisons or the interpretation of individual scale scores. If they are substantial, observed scores can not be interpreted as equally precise administration contexts. In this way, Boot-Err complements Reliability Generalization Meta-Analysis and measurement invariance testing by shifting the focus from ratio-based comparisons and tests for differences to the magnitude of differences in measurement error itself.

## Tutorial

Having demonstrated the conceptual foundations of Boot-Err, we now demonstrate how the method can be implemented in practice. The goal of this tutorial section is to estimate and compare error score variance of the same psychological scale across multiple independent administration contexts and to test and quantify the heterogeneity in random measurement error using Boot-Err.

For illustration, we analyse the Conscientiousness subscale of the HEXACO-personality inventory as administered in the multi-lab replication project reported by Verschuere et al. [-@RRRMazar]. In this project, Experiment 1 from Mazar et al. (ref.) was replicated across 19 independent laboratories. Each lab administered the same materials, including the HEXACO-inventory, to different participant samples scattered across the globe. This data-set therefore provides an ideal setting for Boot-Err: the same scale was administered across multiple independent samples under comparable but not identical conditions.

Importantly, our interest lies not in the experimental manipulation itself, but in the measurement properties of the HEXACO-Conscientiousness scale across laboratories. The HEXACO personality inventory was administered as a filler instrument between experimental tasks - accordingly, scores of Conscientiousness should not be affected by the experimental manipulation. Each lab constitutes a separate administration context, allowing us to examine whether random measurement error varies across contexts.

#### Tutorial structure

This tutorial provides a step-by-step guide on how to investigate differences in measurement quality across administration contexts using Boot-Err. All analyses are conducted in R. The tutorial includes executable chunks that can be run directly on the reader's own machine by copying the code into R. A quarto-file for this tutorial, containing both text and the executable code chunks can alternatively be found at \[.github / OSF-link\]. No additional proprietary software or supplementary materials are required. The data used within this tutorial is publicly available, we make use of an R-package to download and read the data-set. The entire procedure can be reproduced using the code presented in this section.

The tutorial is structured in 7 different steps: (0) data preparation, (1) estimating score reliability, (2) estimating error score variance, (3) using bootstrapping to generate standard errors, (4) meta-analysis of error score variance, (5) backtransforming estimates, (bonus) relating error score variance to total score variance.

At the end of this tutorial, readers will be able to

-   estimate error score variance for each administration context,

-   approximate the sampling variability of transformed error score variance using bootstrapping,

-   test whether error score variance differs across administration contexts, and

-   evaluate the magnitude of these differences in substantive terms

### Step 0 - Preparing Data

Since each data-set is different, depending on how data was collected and possibly aggregated, Step 0 is not formalized in the Boot-Err approach. Instead, the data needs to be cleaned and appropriately manipulated beforehand. By default, the Boot-Err approach makes use of list-wise deletion, if any observation for an item is missing. This tutorial makes use of data in a wide format. Concerning list-wise deletion, this implies that if a participant fails to respond to one or several items from the measuring instrument, all data from that participant is disregarded. Essentially, the entire row from the wide-format data-set is deleted/ignored. Researchers that prefer to make use of more sophisticated approaches to missing data, such as multiple imputation, will need to perform this within step 0.

The following code chunk manages the packages required to follow this tutorial. By executing the code below, the user checks whether the required packages (magrittr, dplyr, osfr, here, zip, data.table, metafor, boot) are already installed. If the packages are not yet installed, they will be downloaded and installed from the default CRAN-repository. All required packages are subsequently loaded.

```{r, output = F, warning = F}
# relevant libraries required for this script
packages <- c("magrittr", "dplyr", "osfr", "here", "zip", "data.table", "metafor", "boot")

# check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  # check if package is installed on system
  pkg_avail <- nzchar(system.file(package = x))   
  
   # load the package, if already installed
  if(pkg_avail){
    require(x, character.only = TRUE)            
    
  # install the package, if missing
  }else{
    install.packages(x)      
    
    # load after installation
    require(x, character.only = TRUE)             
  }
})
```

After loading the required packages, we can turn towards downloading the data-set from the OSF-repository. While the data cleaning and manipulation procedure is bound to be different for every research project, we briefly discuss how we come to a final data-set for this tutorial. Firstly, we download the zipped replication data-set from the openly available OSF-repository (osf.io/fwnc2), using the osfr R package. After unzipping, we can read the data-file in R.

```{r, warning = F, eval = F}
# create relevant directories, so data can be downloaded and stpred
dir.create(here("Data"), showWarnings = FALSE)
dir.create(here("Data/Downloaded Data"), showWarnings = FALSE)


# use osfr-package to download RRR10 data on the Mazar et al. / Srull et al. replications
osfr::osf_retrieve_file("https://osf.io/fwnc2") %>% 
  osfr::osf_download(path = here("Data/Downloaded Data/"), conflicts = "overwrite")
# zip-file is stored at Downloaded Data

# again, unzip the file and move it to the corresponding directory
unzip(here("Data/Downloaded Data/Meta-Analysis_2018-07-09.zip"),
      files = "Meta-Analysis/Results/raw_data_corrected_MAA.csv",
      exdir = here("Data/Original Data/RRR10"),
      junkpaths = T)
```

For the purpose of this tutorial, we want to restrict our analyses to only those participants, which were included in the replicators' primary analyses. Therein, we end up with 4,674 participants from 19 laboratories. We achieve this in substep *a)* by identifying the relevant laboratories from the published manuscript and filtering out all laboratories that do not belong to the primary replication.

```{r, warning = F}
# HEXACO Conscientiousness

# a) Load data and select laboratories part of primary replication

# Thankfully, for the RRR10 project, the data can be taken from a file directly, without the need of combination:
pc_df <- as.data.frame(fread(here("Data/Original Data/RRR10/raw_data_corrected_MAA.csv")))

# data <- data[which(data$age >= 18 & data$age <= 25),]
# retain observations only for participants eligible for analysis
pc_df <- pc_df[which(pc_df$inclusion == "inclusion both RRR" | pc_df$inclusion == "inclusion Mazar only"),] %>%
  mutate(source = lab.name)

# identify relevant labs for analsysis
labs_in_paper <- c("Laine", "klein Selle & Rozmann", "Aczel", "Ferreira-Santos", "Meijer", "Loschelder", "Wick", "Suchotzki", 
                   "Sutan", "Vanpaemel", "Verschuere", "Wiggins", "Gonzalez-Iraizoz", "Koppel", "Birt", "McCarthy", "Evans", 
                   "Holzmeister", "Ozdogru")
labs_in_data <- unique(pc_df$source)
labs_in_data[8] <- "Gonzalez-Iraizoz"
labs_in_data[16] <- "Ozdogru"

# remove labs from data, which we do not need for analysis
labs_excl <- labs_in_data[!labs_in_data %in% labs_in_paper]
pc_df <- pc_df[which(!pc_df$source %in% labs_excl),]

# # include only participants in cheat condition (design was 2x2, cheat - no cheat x commandment - books)
# pc_df <- pc_df[which(pc_df$maz.cheat.cond == "cheat"),]

```

The data-set contains a large amount of information, that we do not need, in the form of columns not required for this analysis. Therefore, in substep *b)* we retain only those columns from the data-set, that contain information about the participants' responses to the HEXACO-items and in which laboratory the participant was assessed. Since some of the HEXACO-items are reverse-coded, we need to re-code half of the items, so the scale is identical across all items.

```{r, warning = F}
# b) Select relevant variables and re-code items that are coded in the opposite direction

# retain only those columns, which are related to the HEXACO-personality inventory
# we can do so by only retaining columns, whose names start with "hex"
# As we need to differentiate between scores from laboratories, we keep column "source"
pc_df <- pc_df[,c(which(names(pc_df) %in% c("source")),
                  grep("^hex", names(pc_df)))]

# recoding the hexaco items, that need recoding

for(i in grep("^hex", names(pc_df))){
  pc_df[,i] <- as.integer(pc_df[,i])
}

# these are the numbers of the items, that need recoding
items_hex_recode <- c(30, 12, 60, 42, 24, 28, 53, 35, 41, 59, 28, 52, 10, 46, 9, 15, 57, 21, 26, 32, 14, 20, 44, 56, 1, 31, 49, 19, 55, 48)

# combining "hex" and number gives the column names
names_items_hex_recode <- paste0("hex", items_hex_recode) 

# adding _R for names of items, that are recoded
names_items_hex_recode_R <- paste0(names_items_hex_recode, "_R") 

# replace the original (un-recoded) scores with the recoded ones
pc_df[,names_items_hex_recode_R] <- 6 - pc_df[,names_items_hex_recode] 
```

Lastly, in substep *c)*, we remove all columns that contain responses on the other HEXACO-dimensions, which are not Conscientiousness. Therein, we have constructed a data-set, containing the responses on Conscientiousness from all participants that are part of the primary replication sample in RRR10.

```{r, warning = FALSE}
# c) select items belonging to Conscientiousness subscale

# Conscientiousness
# items in Conscientiousness subscale
items_hex_CO <- c(2, 26, 8, 32, 14, 38, 50, 20, 44, 56) 

# did item need recoding?
names_items_hex_CO <- ifelse(items_hex_CO %in% items_hex_recode, paste0("hex", items_hex_CO, "_R"), paste0("hex",items_hex_CO)) 

# select all items from Conscientiousness subscale, correctly coded
pc_hex_items_CO <- which(names(pc_df) %in% names_items_hex_CO) 

# retain only columns concerning Conscientiousness and source
pc_df <- pc_df[,c(names_items_hex_CO, "source")]

# identify individual laboratories
labs <- unique(pc_df$source)
```

However, we find that some participants' responses on the Conscientiousness-items are missing in this data-set. While this was of no concern to the replication-authors, as the HEXACO-inventory served as a filler instrument, the same is not true for this tutorial. We will use Cronbach's Alpha to estimate score reliability. Cronbach's Alpha can not deal with missing responses. To keep it simple for this tutorial, we make use of list-wise deletion, removing any missing responses from the data-set. This means that the data for each participant where any response on one or several items is missing is deleted from the data-set. Alternative approaches, that attempt to impute the missing responses while retaining the general covariance-structure of the data-set are available. Great examples include Multiple Imputation (ref.) or Multivariate Imputation by Chained Equations (ref.). Singe imputation methods like Mean or Mode Imputation restrict the data-sets covariance structure, leading to estimation issues and biases in subsequent analyses.

In the following code chunk, we compute how many scores are missing for each individual item. Subsequently, we retain only scores from participants, where no single item score for Conscientiousness is missing.

```{r, warning = FALSE}
# d) dealing with missing responses

# compute number of missing values per item
colSums(is.na(pc_df))

# retain only complete observation sets
pc_df <- na.omit(pc_df)
```

Finishing step 0, we have constructed a data-set, containing only the complete response sets of participants belonging to the replication authors' primary analysis sample. Subsequently, we can begin to follow the Boot-Err procedure.

### Step 1 - estimating score reliability

The first step in the Boot-Err procedure aims to generate estimates of score reliability for each sample or administration site respectively. In the following code, we use *sapply()* to compute Cronbach's Alpha separately for each laboratory in the data-set. Cronbach's Alpha is generally defined as outlined in the following @eq-alpha_formal (ref.).

$$\alpha=\frac{k^2\ \bar{\sigma}_{ij}}{\sigma^2_x}$$ {#eq-alpha_formal}

Here, $k$ corresponds to the number of items in the scale, $\bar{\sigma}_{ij}$ constitutes the average covariance between items and $\sigma^2_x$ constitutes the total score variance in the total test results.

However, computing the average item covariance can be computationally intensive, and a more efficient computation technique is available in @eq-alpha_fast (ref.).

$$\alpha = \frac{k}{k-1}\left(1-\frac{\sum_{i=1}^k\sigma^2_{i}}{\sum_{i=1}^k\sum_{j=1}^k\ \sigma_{ij}}\right) $$ {#eq-alpha_fast}

Here, $\sigma^2_{i}$ describes the variance of scores on item $i$, while $\sigma_{ij}$ describes the covariance between items $i$ and $j$. The equation in @eq-alpha_fast generates the same estimate of Cronbach's Alpha as in @eq-alpha_formal, but much more efficiently. By using *sapply* to quickly compute across all laboratories, we can efficiently generate a vector containing estimates of Cronbach's Alpha for each administration site respectively.

```{r, warning = F}
# compute Cronbach's Alpha across all administration contexts

# we use sapply as it tends to be faster than a for-loop
alpha_vec <- sapply(labs, FUN = function(lab){
  
  # select data for each administration context separately
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  # compute covariance matrix
  C <- cov(dat)
  
  # extract the number of items
  j <- dim(C)[1]
  
  # compute Cronbach's Alpha efficiently
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  return(alpha)
})

# return the estimate vector (rounded to 2 decimals)
round(alpha_vec, 2)
```

### Step 2 - estimating error score variance

After generating estimates of score reliability using Cronbach's Alpha, we can turn towards Step 2 of the Boot-Err procedure - estimating error score variance. In @eq-errorvar we established that error score variance can be constructed from a reliability coefficient and the total score variance. This means, provided we generate an estimate of total score variance, we can use @eq-error_est to estimate error score variance, as we already have estimate of score reliability in the form of Cronbach's Alpha readily available.

In the following code we, again, use an *sapply*-function to efficiently repeat the same computation for each laboratory. We estimate the total score variance $\sigma^2_X$ by computing Conscientiousness scores as the average item score for each participant (generating the test score) and computing the variance of these test scores. Subsequently, we can easily use @eq-errorvar to generate a vector containing error score variance $\sigma^2_E$-estimates for each laboratory respectively.

```{r}
# estimate error score variance for each administration context

# we use sapply as it tends to be faster than a for-loop
var_X_vec <- sapply(labs, FUN = function(lab){
  
  # select data for each administration context separately
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  # compute the total score variance from individual test scores
  var_X <- dat %>% 
    rowMeans(.) %>% 
    var(.)
  
  return(var_X)
  
})

# compute error score variance from the estimates of reliability and total score variance
var_E_vec <-  (1-alpha_vec) * var_X_vec

# return the estimated vector (rounded to 3 decimals)
round(var_E_vec, 3)

```

### Step 3 - construct Standard Errors using Bootstrapping

As described in the introductory *Boot-Err*-section, we use a random-effects meta-analysis to estimate and test for heterogeneity in error score variances. Since estimation of total score variance and its components is not variance-stable, we need to use a variance-stabilizing transformation. Bartlett and Kendall [-@bartlett1946] show that using the natural log, as in @eq-stable_var, is a suitable variance-stabilizing transformation for total score variance estimates. 

$$ T[\hat{\sigma}^2_x] = ln \left(\hat{\sigma}^2_x \right) $$ {#eq-stable_var}

While little evidence on variance-stabilizing transformations for score variance components is available, we have used the natural log for both the total and the error score variance component in our proposal for the Boot-Err [@beinhauer2025]. In the simulation scheme, we only a small but negligible bias in the estimates. We concluded that the transformation described in @eq-stable_var is suitable for the variance component of error score variance $\sigma^2_e$. Therefore, we are confident in our claim that @eq-stable_varE describes a suitable variance-stabilizing transformation.

$$ T[\hat{\sigma}^2_e] = ln \left(\hat{\sigma}^2_e \right) $$ {#eq-stable_varE}

However, in order to run a random-effects meta-analysis, we not only need a variance-stable estimate, but also a Standard Error, quantifying our uncertainty in that estimate. While we are aware of no analytic approximation for the Standard Error of log-transformed error score variance, we can use Bootstrapping techniques to generate robust Standard Error estimates for $T[\hat{\sigma}^2_e]$.

The package *boot* provides easy-to-use and efficient functions (including parallelization if desired) for Bootstrapping techniques. During the conception of this tutorial, we made use of version 1.3-31. In order to use *boot*, we need to provide the package with a user-written function, which needs to do two things: i) allow the *boot*-package to sample from our data-set (we do so by `dat_boot <- data[indices,]`), and ii) compute the parameter on the re-sampled data-set that we want to boot-strap. In the following code-chunk, we show a function that does just that.

```{r}
# define the user-written function to compute transformed error score variance

ln_var_E_boot_func <- function(data, indices){
  
  # essential for any boot-function: allow boot to sample from dataset rows
  dat_boot <- data[indices,]
  
  # we have to compute Cronbach's Alpha again for each bootstrapped sample
  
  # compute covariance matrix
  C <- cov(dat_boot)
  
  # extract number of items
  j <- dim(C)[1]
  
  # compute Cronbach's Alpha
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  # we also have to compute error score variance again for each bootstrapped sample
  
  # compute total score variance
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  # compute error score variance
  var_E <- var_X * (1 - alpha)
  
  # log-transform as variance stabilising transformation
  ln_var_E <- log(var_E)
  
  return(ln_var_E)
  
}
```

Now that we have a function available for *boot* to re-sample our data and estimate $T[\hat{\sigma}^2_e]$ in the bootstrapped samples, we need to supply our user-defined function to *boot* and compute the Standard Error from the bootstrapped estimates. Essentially, what we do here is estimate the Standard Error of log-transformed error score variance, by computing the standard deviation of all bootstrapped estimates for a single laboratory, see @eq-bootSE.

$$ SE\left[T[\hat{\sigma}^2_e]\right] = \sqrt{\frac{1}{n}\sum_{i=1}^n(T[\hat{\sigma}^2_e]_i - \bar{T}[\hat{\sigma}^2_e])^2}$$ {#eq-bootSE}

Here, $n$ describes the number of bootstrap-samples drawn, whereas $\bar{T}[\hat{\sigma}^2_e]$ describes the mean of the log-transformed error score variance estimates of the $n$ bootstrap samples. In the following code-chunk, we use the *boot* package to estimate $SE\left[T[\hat{\sigma}^2_e]\right]$ from 3000 bootstrap samples for each laboratory respectively. Again, we make use of an *sapply*-function to efficiently compute across all laboratories.

```{r, warning = FALSE}
# approximate the standard error for error score variance

ln_var_E_SE_vec <- sapply(labs, FUN = function(lab){
  
  # select data for each administration context separately
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  # use boot and our user-defined function to efficiently generate transformed error score variance in 3000 bootstrapped samples
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_E_boot_func,
                               R = 3000)
  
  # compute the standard error
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})
```

Lastly, we still need to formally transform and store the individual estimates of $T[\hat{\sigma}^2_e]$ for each laboratory (so far we only did this for the bootstrap-samples). However, at this point, computing this transformation is trivial. To make things easier for us in the following steps, we combine the estimates of log-transformed error score variance and its Standard Errors here in a single data-frame. We add the laboratory names to the data-frame, so estimates can be linked to their respective sources.

```{r, warning = FALSE}
# combine standard errors and estimates in a data-frame

# transform individual estimates of error score variance
ln_var_E_vec <- log(var_E_vec)

# combine estimates, standard error and lab-name in data-frame
ln_var_E_df <- data.frame(ln_var_E = ln_var_E_vec, 
                          ln_var_E_SE = ln_var_E_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

# return the estimates and round the numeric variables (ln-estimate and SE)
ln_var_E_df %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))
```

Concluding step 3, we have gathered all estimates we need, to perform a random-effects meta-analysis on (log-transformed) error score variances.

### Step 4 - meta-analysis of error score variance

For this step, we estimate the heterogeneity of log-transformed error score variance and test whether it can be statistically distinguished from zero. We use the R-package *metafor*, as it provides an efficient and easy-to-use implementation of random-effects meta-analyses for any parameter [@metafor]. Using the *rma*-function with the argument *method = "REML"* ensures that we use the REML-estimator of heterogeneity. Hönekopp and Linden [-@hoenekopp2022] have shown that the REML-estimator tends to outperform other estimators across a wide range of conditions. 

The following chunk will run a random-effects meta-analysis in *metafor*, generating an estimate of the meta-analytic mean (in the output `estimate`), estimates of absolute heterogeneity in standard deviation and variance (in the output `tau` and `tau^2`) and estimates of relative heterogeneity in the form of $I^2$ and $H^2$ (in the output `I^2` and `H^2`). Additionally, *metafor* provides the results for two hypothesis tests. Firstly, the test for heterogeneity (`Test for Heterogeneity` in the output), assessing whether the estimate of heteorgeneity $\tau^2$ can be statistically distinguished from $0$. Secondly, the test for the meta-analytic effect size, assessing whether the meta-analytic aggregate can be statistically distinguished from $0$ (in the Output `Model results`).

```{r}
# perform the random-effects meta-analysis on log-transformed error score varice
ln_var_E_rma <- metafor::rma(data = ln_var_E_df,
                             yi = ln_var_E,
                             sei = ln_var_E_SE,
                             method = "REML")

ln_var_E_rma

```

The significance tests indicate two things for the log-transformed error score variance of Conscientiousness across replications in Verschuere et al. [-@RRRMazar]: Firstly, we find that the meta-analytic estimate of `r paste0("$\\mu_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$b[1], 3), "$")` is significantly different from zero (`r paste0("$z = ", round(ln_var_E_rma$zval, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$pval < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$pval, 3))), "$")`), using a significance level of 5%. However, this information is not particularly meaningful, the estimate is on the log-transformed scale and interpretation is not straightforward. Secondly and more interestingly, we find that the test for heterogeneity indicates that there is statistically significant heterogeneity in log-transformed error score variances (`r paste0("$Q(", ln_var_E_rma$k - 1, ") = ", round(ln_var_E_rma$QE, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$QEp < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$QEp, 3))), "$")`). Even though the tested estimates are still on the log-scale, we can nonetheless infer from this result, that error score variance $\sigma^2_E$ is not identical across all administrations of the Conscientiousness-scale. 

However, the estimate of `r paste0("$\\tau^2_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$tau2, 3), "$")` can not be easily interpreted due to the log-transformation. Instead, the estimates of relative heterogeneity may be interpreted despite the transformation. The estimate of `r paste0("$I^2 = ", round(ln_var_E_rma$I2, 2), "$")` implies that `r round(ln_var_E_rma$I2, 2)`% of the variance observed in the estimate of error score variance are due to actual heterogeneity in the population, not due to sampling error. Similarly, the estimate of `r paste0("$H^2 = ", round(ln_var_E_rma$H2, 2), "$")` implies that the heterogeneity is `r round(ln_var_E_rma$H2, 2)` times as large as the variance induced by sampling error. According to standard conventions (ref.), both estimate imply a strong or large extent of heterogeneity. However, both estimates of relative heterogeneity are often criticized, as they depend strongly on sample size and are easily manipulated (refs.). 

Instead, we should turn towards an informative interpretation of the absolute heterogeneity in terms of $\tau^2$ or $\tau$. To do so, we need to back-transform our newly generated estimates to the original scale. We do so in Step 5.

### Step 5 - Back-transforming estimates

The transformation of score variance (components) using the natural log not only serves to generate variance-stable estimates. Additionally, it comes with the added benefit, that we can assume $T[\sigma^2_e]$ to follow a normal distribution. Several authors have proposed that the total score variance, if not constant, can be described by a log-normal distribution (refs.). This means that the log-transformed estimates accordingly follow a normal distribution. This is very helpful for the application of the Boot-Err technique, as one central assumption of a random-effects meta-analysis is that the parameter follows a normal distribution. Additionally, it helps us back-transform the estimates of meta-analytic mean $\mu_{T[\sigma^2_e]}$ and its heterogeneity $\tau^2_{T[\sigma^2_e]}$, as equations are readily available (refs.).

$$ \hat{\mu}_{\sigma^2_e} = \exp\left(\hat{\mu}_{T[\sigma^2_e]} + \frac{1}{2} \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_mu}

@eq-backtransf_mu demonstrates how the mean error score variance $\hat{\mu}_{\sigma^2_e}$ can be estimated from the meta-analytic estimates of $T[\sigma^2_e]$. Here, $\hat{\mu}_{T[\sigma^2_e]}$ represents the estimate of meta-analytic mean of log-transformed error score variance and $\hat{\tau}^2_{T[\sigma^2_e]}$ describes the estimate of its heterogeneity. Similarly, @eq-backtransf_var can be used to derive the heterogeneity of error score variance $\hat{\tau}^2_{\sigma^2_e}$ on its original scale, from the same meta-analytic estimates of $T[\sigma^2_e]$.

$$ \hat{\tau}^2_{\sigma^2_e} = \exp\left(\hat{\tau}^2_{T[\sigma^2_e]} - 1\right) \cdot \exp\left(2 \hat{\mu}_{T[\sigma^2_e]} + \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_var}

In the following chunk, we define two functions which compute [Equations -@eq-backtransf_mu] and [-@eq-backtransf_var] from the *metafor*-output. The functions only take the *metafor*-object generated from the *rma*-function as input and return the estimates $\hat{\mu}_{\sigma^2_e}$ and $\hat{\tau}^2_{\sigma^2_e}$ respectively.

```{r}
# two functions to back-transform heterogeneity and meta-analytic estimate

# back-transform meta-analytic estimate (the weighted mean)
backtransform_var_mu <- function(rma_obj){
  exp(rma_obj$b[1] + (.5*rma_obj$tau2))
}

# back-transforming meta-analytic heterogeneity
backtransform_var_tau2 <- function(rma_obj){
  (exp(rma_obj$tau2) - 1) * exp((2 * rma_obj$b[1]) + rma_obj$tau2)
}


round(backtransform_var_mu(ln_var_E_rma), 3)

round(backtransform_var_tau2(ln_var_E_rma) %>% 
  sqrt(.), 3)
```

Running the two functions in the chunk above returns two estimates: The meta-analytic mean error score variance of the HEXACO-Conscientiousness scale across 19 laboratories is $\hat{\mu}_{\sigma^2_e} = `{r} round(backtransform_var_mu(ln_var_E_rma), 3)`$; The estimate for its heterogeneity is $\hat{\tau}_{\sigma^2_e} =`{r} round((backtransform_var_tau2(ln_var_E_rma) %>%   sqrt(.)), 3)`$. To understand what this implies, we can assess the relative size of heterogeneity, also known as the coefficient of variation, which is computed as $CV = \frac{\tau}{\mu}$. Here we find a CV of $CV_{\sigma^2_e} = `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3)`$. This means that the extent of the differences in error score variances is approximately `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance found across these laboratories. Based on the random-effects model, true error score variance across administrations can be expected to lie approximately between X and Y. These values can be obtained by adding and subtracting the estimated heterogeneity $\hat{\tau}_{\sigma^2_e}$ from the meta-analytic mean $\hat{\mu}_{\sigma^2_e}$ (on the transformed scale) and subsequently back-transforming to the original variance metric. For Conscientiousness, we find an interval where we expect the error score variance to vary at least between `r round(exp(ln_var_E_rma$b[1] - sqrt(ln_var_E_rma$tau2)), 3)` and `r round(exp(ln_var_E_rma$b[1] + sqrt(ln_var_E_rma$tau2)), 3)`

None of the other approaches, such as RG-MA, IRT or measurement invariance testing allow to generate these estimates we have derived using the Boot-Err technique. For researchers who have experience working with the respective scale, these estimates can be highly valuable to understand how much variance in scores we can expect to be introduced by measurement imprecision across multiple administrations of the same scale. While we, as authors, lack the expertise on what to expect from the conscientiousness scale, we can construct additional parameters to gain a deeper understanding. In the following Bonus-Step, we delve deeper into the insights generated by the Boot-Err technique.

### Bonus-Step - Relating error score variance to total score variance

Score reliability $\rho_{xx'}$ is defined as the relative extent to which total score variance observed is attributable to actual differences in the true scores, not to measurement imprecision, see @eq-reliability. However, that also means that $1 - \rho_{xx'}$ describes the relative extent to which total score variance is attributable to random measurement error.

Similarly, on the meta-analytic scale, we can look at the heterogeneity in total score variance $\sigma^2_x$, and attempt to understand how much of its heterogeneity can be attributed to differences in error score variance. Intuitively, many researchers tend to ascribe heterogeneity in total score variance $\sigma^2_x$ to differences in measurement precision. This means that often, a larger score variance is interpreted as if it was caused by larger measurement error such as $\sigma^2_e$. However, substantial parts of said heterogeneity might also be better explained by differences in the true score variance $\sigma^2_t$. Fortunately, if we follow Steps 3 through 5 for the total score variance $\sigma^2_x$ instead, we can assess in how far its heterogeneity can be attributed to differences in true score variance $\sigma^2_t$ or measurement error $\sigma^2_e$.

In the following chunk, we briefly run through these steps to estimate heterogeneity in total score variance $\sigma^2_x$.

```{r}
# perform steps 3 through 5 for total score variance

# define the function for boot to bootstrap transformed total score variance
ln_var_X_boot_func <- function(data, indices){
  
  # allow boot to resample from data-set
  dat_boot <- data[indices,]
  
  # compute total score variance
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  # transform total score variance
  ln_var_X <- log(var_X)
  
  return(ln_var_X)
}


# use Bootstrapping to compute standard error of log-transformed total score variance
ln_var_X_SE_vec <- sapply(labs, FUN = function(lab){
  
  # select data only for specific administration context
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  # use boot to bootstrap log-transformed total score variance
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_X_boot_func,
                               R = 3000)
  
  # estimate standard error
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})

# transforme total score variance estimates
ln_var_X_vec <- log(var_X_vec)

# combine estimates of total score variance and its standard error in single df
ln_var_X_df <- data.frame(ln_var_X = ln_var_X_vec, 
                          ln_var_X_SE = ln_var_X_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

# run random-effects meta-analysis
ln_var_X_rma <- metafor::rma(data = ln_var_X_df,
                             yi = ln_var_X,
                             sei = ln_var_X_SE,
                             method = "REML")

# round back-transformed meta-analytic estimate to 3rd decimal
round(backtransform_var_mu(ln_var_X_rma), 3)

# round back-transformed heterogeneity estimate to 3rd decimal
round(backtransform_var_tau2(ln_var_X_rma) %>% 
  sqrt(.), 3)

```

We find a meta-analytic mean of total score variance $\hat{\mu}_{\sigma^2_x} = `{r} round(backtransform_var_mu(ln_var_X_rma), 3)`$ and a heterogeneity of $\hat{\tau}_{\sigma^2_x} = `{r} round((backtransform_var_tau2(ln_var_X_rma) %>% sqrt(.)), 3)`$. Under CTT, total score variance is defined as the sum of true and error score variance $\sigma^2_x = \sigma^2_e + \sigma^2_t$, which are assumed to be independent. This means that we can assess how much heterogeneity in total score variance can actually be explained by differences in measurement error:

$$ R = \frac{\hat{\tau}^2_{\sigma^2_e}}{\hat{\tau}^2_{\sigma^2_x}} $$ {#eq-ratio}

In @eq-ratio, we define the ratio-variable $R$ as the ratio of $\hat{\tau}_{\sigma^2_e}$ to $\hat{\tau}_{\sigma^2_X}$. For Conscientiousness, we find a ratio of $R  = `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3)`$. This means that about `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3) * 100`% of heterogeneity in $\sigma^2_x$ could be explained by differences in random measurement error. This also means that `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`% of this heterogeneity is explained by differences in true score variance $\sigma^2_t$.

By following the steps from 1 through 5 we have assessed differences in measurement quality of Conscientiousness across different administration contexts. In doing so, including the Bonus-Step, we have generated a number of estimates and parameters that help us understand in how far these differences are substantive. In the following section, we turn towards a substantive interpretation of what these results imply for the use of the HEXACO-Conscientiousness inventory.

### Conclusion

Boot-err essentially consists of 5 steps, where we estimate score reliability (step 1), estimate error score variance (step 2), use appropriate variance-stabilizing transformations paired with Bootstrapping to estimate standard errors (step 3), perform meta-analyses of error score variance (step 4) and back-transform estimates back to the original scale (step 5). In the previous section, we provided custom functions in R for researchers to perform a Boot-Err analysis on data, obtained from a single measuring instrument administered across different contexts. Combining the individual steps allows for an assessment of whether there is heterogeneity in random measurement error across administrations of the same scale. 

Making use of the data supplied by Verschuere et al. [-@RRRMazar], we used Boot-Err to understand in how far there are differences in absolute measurement precision across 19 laboratories where the HEXACO-Conscientiousness personality scale was administered. We not only find statistically significant differences in error score variance, but were also able to estimate the absolute and relative extent of these differences. Therein, we are certain that the random measurement error in the Conscientiousness scores is not constant across samples. We find that the heterogeneity of `{r} round(backtransform_var_tau2(ln_var_E_rma), 3)` is about `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance. Across administrations of the same scale, we can expect error score variance to vary at least between `r round(exp(ln_var_E_rma$b[1] - sqrt(ln_var_E_rma$tau2)), 3)` and `r round(exp(ln_var_E_rma$b[1] + sqrt(ln_var_E_rma$tau2)), 3)`.

In order to better understand what these differences mean, we additionally uncovered in how far the heterogeneity in total score variance can be attributed to these differences in measurement error. Here, we found that the overwhelming majority of heterogeneity, `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`%, can not be explained by differences in measurement error. For the Conscientiousness-scale, this implies that, while there clearly are substantial differences in measurement error across groups, the distribution of true scores varies much more strongly across groups. This is important, as this implies that the differences in total score variance can not be attributed to differences in measurement error. Instead, Conscientiousness is dispersed highly different across those groups. In Beinhauer et al. [-@beinhauer2025liable] we discuss potential causes and implications of this phenomenon.

## Discussion

Measurement precision and how strongly it diverges across administrations of the same instrument is a central question in social and behavioural sciences. Differences in random measurement error are not only interesting to researchers assessing the quality of a measuring instrument. These differences also have clear implications concerning hypothesis testing and standardization of effect sizes. We have summarised some of the most popular techniques assessing differences in random measurement error - Reliability Generalization Meta-Analysis and Measurement Invariance Testing. Additionally, we introduced Boot-Err, a technique we developed to step away from relative measures such as score reliability or tests without estimates of how large the differences are, like Measurement Invariance Tests [@beinhauer2025].

In the tutorial-section, we have demonstrated how Boot-Err can be used to generate detailed estimates and tests regarding the extent of the differences we observe in random measurement error, and how strongly they affect the differences in score variance. It is important to note that this technique comes with a number of limitations and constraints to generalizability. Just like for any statistical analysis, a number of assumptions need to be met. Regarding the application of Boot-Err, total and error score variance should follow a log-normal distribution and be computed from samples that are generally independent of each other (the individual samples should not be described as if stemming from a nested structure). Additionally, a number of assumptions relate to the type of score reliability estimate used. Here, we made use of Cronbach's Alpha, which typically comes with assumptions of tau-equivalence and unidimensionality (ref.). While these assumptions are widely criticized and rarely hold up in real data (ref.), we need to remind ourselves that these assumptions also relate to typical scoring methods of these measuring instrument. The HEXACO-personality score for each dimension is computed by taking the average of the relevant item scores (ref. Hexaco). Therein, even stricter assumptions going beyond tau-equivalence and unidimensionality apply - making Cronbach's Alpha the more appropriate choice of score reliability estimate over other ones that may impose less strict assumptions.

Even though differences in absolute measurement precision are of central interest to measuring instrument and scale development, methods to assess differences in error score variance each come with their own caveats and shortcomings. Models regarding differences in score reliability fail to take differences introduced by true score variance into account, while models simply testing for differences in error score variance without actually estimating it fail to put these differences into perspective. Perhaps this is the reason why discussions surrounding the impact of random measurement error on standardized effect sizes and hypothesis tests has been overemphasized (refs. \[NEED TO PICK THIS UP IN INTRO\]), implying that removing random measurement error was akin to harmonizing variability across populations and data-sets.

Boot-Err, mends both these shortcomings, as differences in error score variance can be estimated and tested, but also put into perspective on how much heterogeneity in total score variance they actually produce. As we demonstrated in the Bonus-Step section, differences in error score variance can not explain why total score variance differs across groups--instead, differences in true score variance must come at a much larger extent. This implies that differences in score variability can not sufficiently be mended by taking care of measurement error. Instead, future research should ensure that the true score variance is stable across groups and representative for the respective population.

#### Abstellgleis

DAS HIER KÖNNT IHR IGNORIEREN - DAS SIND ALTE TEXTBAUSTEINE DIE FÜR DEN MOMENT KEINEN PLATZ HABEN

One central assumption of IRT is that the item characteristics are valid for all subgroups in the population to which the model is applied. This means that, no matter the sub-group the individual is from, the item-response-function should be identical. This means that all individuals with the same latent ability score $\theta$ have the same probability of getting an item right or wrong. As this is a central assumption of these models, routine procedures for examining differential item functioning (DIF) have been established. Typically, a *reference sample* should be available, where an IRT-scoring model has been established. Subsequently, we can assess if any items exhibit DIF in a *focal group*, where the same $\theta$ would lead to different observed scores.

## References

::: {#refs}
:::
