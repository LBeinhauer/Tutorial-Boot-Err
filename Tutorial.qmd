---
title: "Tutorial"
format: html
bibliography: bibliography/Boot-Err_Tutorial.bib
csl: bibliography/apa-6th-edition.csl
---

# New

Intro - initial question / that is best answered using Boot-Err

Notes Brainstorming Jens:

-   was kann RGMA nicht? Prädiktoren hinzufügen!

    -   Shaw etc! Prädiktoren

Perspektive:

Messinstrument?

Standardisierte Effekte?

-   Forschungsfrage pro Methode vs. unterschiedliche Definition pro Methode

-   Wenn Rel immer gleich, aber varianzen unterschiedlich, dann muss das Heteroskedastizität implizieren?

-   partial pooling um schätzungen der "wahren" Fehlervarianzen zu generieren

Jens Fünderich, 2026: Punktschätzungen der wahren Fehlervarianz

## Introduction

Researchers frequently administer the same psychological scale across different samples, settings or populations. A scale may be translated into another language, administered online instead of in person, or completed by participants with different educational or cultural backgrounds. In such situations, an important question arises: does the instrument produce scores of equivalent measurement quality across these administrations?

At first glance, this question appears straightforward. However, what is meant by measurement quality is less obvious. Throughout this tutorial, we focus on random measurement error as a central component of measurement quality. Random measurement error refers to unsystematic fluctuations in observed scores around the values they would take on in the absence of measurement imprecision. Unlike systematic error, which shifts observed scores in a particular direction, random error introduces general fuzziness, often described as noise. On average, observed scores may still be accurate, but individual measurements vary unpredictably around their underlying true values.

If the same instrument is administered in two different contexts, differences in random measurement error imply differences in measurement quality. However, there are several ways to conceptualize and quantify such differences. Importantly, these approaches do not contradict each other. Rather, they reflect different perspectives on what it means for measurement quality to be equivalent.

The first, most intuitive and widely used answer is to compare reliability coefficients. Reliability Generalization Meta-Analysis extends this idea by formally testing whether oberved differences in score reliability exceed what would be expected by sampling error. A second, more structurally explicit answer is provided by measurement invariance testing. This framework models the relation between latent factors and observed item responses and evaluates whether residual variances (variance not explained by the underlying factor) are equivalent across groups. Finally, Boot-Err introduces a third perspective. It conceptualises measurement quality directly as error score variance and focuses on estimating and comparing this variance component across administrations. While all three approaches address the question of whether measurement quality differs, they do so from conceptually distinct perspectives and therefore lead to different interpretations.

Throughout this tutorial, we will explore the perspectives and interpretations from the three approaches using an explicit example. Consider a scale measuring Extraversion administered to a wide range of different samples, under which we find a German sample of Psychology students and an international online panel for market research. The concept of Extraversion is thought to be generalizable across different populations and potentially linked to a larger number of different effects and phenomena. To explore these links, however, we need to make sure that we are measuring comparably well across all administration sites - otherwise we cannot explore to what extent Extraversion actually generalizes across contexts and to what extent the links to other effects and phenomena hold up across contexts.

### Reliability generalization meta-analysis

A simple and accessible way to compare measuring quality across administrations of the same scale is by comparing the reliability coefficients computed for each administration sample. Reliability coefficients, such as Cronbach's Alpha, McDonald's Omega or Guttmann's Lambda IV attempt to quantify the proportion of true score variance present in the scores:

$$\rho_{XX'} = \frac{\sigma^2_T}{\sigma^2_X}$$ {#eq-reliability}

where $\rho_{XX'}$ is score reliability, $\sigma^2_T$ is the true score variance and $\sigma^2_X$ is the total score variance, made up of true and error score variance ($\sigma^2_X = \sigma^2_T + \sigma^2_E$, where $\sigma^2_E$ is the error score variance).

This true score variance $\sigma^2_T$ describes the variation in the scores, that can be attributed to genuine differences in the underlying factor being measured. For the purpose of this tutorial, we will refer to it as *signal* - the part of the score that we actually intended to measure. Error score variance $\sigma^2_E$ on the other hand describes the variation in the scores, that can be attributed to random, unsystematic variation - random measurement error that is introduced by the instruments inability to perfectly measure the underlying factor, which we will refer to as *noise* for the purpose of this tutorial. Thereby, these reliability coefficients essentially describe which proportion of the observed score variance can *not* be attributed to noise. Differences in reliability coefficients therefore highlight, that the proportion of signal to total variation in the data differs across administration sites.

Generally, reliability coefficients can take on values between 0 and 1, where 0 indicates that the variance in observations is made up of 100 % noise, while a 1 indicates that there is no noise in the data. Returning to our example, assume we find a Cronbach's alpha .67 for the scores obtained from our sample of Psychology students, but a Cronbach's alpha of .75 for observations stemming from our online panel. Comparing these numbers, we can conclude that the proportion of signal to total variation in the scores is worse for data obtained from Psychology students than for data obtained from the online panel. Of the total score variance observed in each individual sample, a higher proportion can be attributed to true differences in Extraversion for the online panel, than for the student sample.

However, coefficients derived from samples are always prone to sampling error. We are aware of this truth everytime we make use of a test for statistical significance of a mean value or standardised effect size (or estimate a posterior distribution for mean value / effect size). Just like the population mean value is not necessarily identical to the estimate obtained from the sample, the same is true for the estimated reliability coefficient. Due to the randomness in the sampling process, the estimated mean value or reliability coefficient may vary randomly around the population value.

This means that we can not simply compare the estimates of reliability coefficients to determine whether there are true differences in measurement precision across administration sites. However, a formal method to assess whether reliability coefficients of the same instrument differ across administrations has been developed in the form of reliability generalization meta-analysis. Comparing and combining estimates from several studies or samples to derive appropriate mean values and compute how strongly the population values underlying these estimates diverge is known as research synthesis. Techniques in the space of research synthesis typically attempt to model the sampling error introduced by the randomness of the sampling procedure, and correct for it when testing statistical significance or generating estimates. Meta-analysis is a technique that incorporates the uncertainty of sample estimates and allows to test for and model the variance that goes beyond the sampling error. This variance, referred to as heterogeneity, describes the variation of the coefficient that we would have observed, if there was no sampling error. If significant heterogeneity is detected, this indicates that reliability differs across adminstrations beyond what would be expected from sampling fluctuations alone. However, it does not identify the source of these differences.

Reliability Generalization Meta-Analysis aggregates estimates of the same reliability coefficient such as Cronbach's Alpha across several administrations of the same scale, to test whether there are statistically significant differences between these coefficients. Additionally, an estimate of heterogeneity can be derived, which describes how strongly the reliability coefficients vary. In our example case, we can compute Cronbach's Alpha (or other reliability coefficients) for our Psychology student sample, for our market research panel and for the other administrations of the scale that we want to compare. Reliability Generalization Meta-Analysis can inform us whether the differences in Cronbach's Alpha are indeed statistically significant. Thereby, we can properly test whether measurement precision in terms of score reliablity differs across administrations.

Score reliability is the ratio of signal to total score variance, as demonstrated in @eq-reliability. The total score variance is essentially made up of the sum of signal and noise. This means that differences in reliability coefficients can not necessarily be attributed to differences in the noise - this is true irrespective of which reliability coefficient is computed. In our example we assumed that score reliability in the sample of Psychology students was .67, while score reliability in the online market research panel was .75. We initially interpreted this as an indicator of impaired measurement quality for the Psychology students' Extraversion, implying larger noise in that sample, but what if the differences actually lie in the signal? Assume that for both samples actually a noise or error score variance of 1 was underlying. It is reasonable to assume that a sample of German Psychology students is put together more homogeneously than an online panel for market research that is deliberately constructed to represent a more diverse population. Therefore, the signal or true score variance might have actually been 2 for the Psychology student sample and 3 for the market research panel. Such a pattern could explain the differences in score reliability irrespective of differences in noise:

$$\rho_{students} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{2}{2 + 1} = .67 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rho_{panel} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{3}{3 + 1} = .75$$ {#eq-RGMA1}

Alternatively, imagine that the noise or error score variance for the online panel might have been 1.5 instead of 1. In that case, the following pattern would emerge:

$$\rho_{students} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{2}{2 + 1} = .67 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rho_{panel} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \frac{3}{3 + 1.5} = .67$$ {#eq-RGMA2}

In this case, reliability coefficients would be have been identical across both samples, even though noise varied across samples.

Both examples demonstrate that reliability coefficients are affected by differences in signal just as much as on differences in noise. They are a relative coefficient, informing us about the relative amount of signal in the total variation. Only if we have reasons to assume that the signal variance should be identical across administrations, then we can interpret results of Reliability Generalization Meta-Analysis as indicative of differences in measurement quality. Consequently, Reliability Generalization Meta-Analysis answers a ratio-based question about measurement quality, not a signal- or noise-based question.

### Measurement invariance testing

An alternative perspective on measurement quality is offered through Measurement Invariance Testing. Measurement Invariance Testing is grounded in Confirmatory Factor Analysis, typically modelled using Structural Equation Modelling. The analyses previously discussed are grounded in classical test theory, implying that each score $X$ is made up of true score $T$ and error score $E$. Measurement Invariance Tests, on the other hand, are based on the modelling of latent variables. Its aim is to model whether the relationship between latent factors and observed item responses is identical across different groups where the same measuring instrument is administered.

Different levels of measurement invariance exist, and in a series of four (sometimes more) steps, different levels are tested. The four levels can be summarised as:

-   Configural invariance: Across all administrations, the measuring instrument assesses the same number of factors. If several factors are assessed, the individual items relate to the same factors across all administrations.

-   Metric invariance: The factor loadings for each individual item are identical across all administrations.

-   Scalar invariance: Systematic differences in the observed scores between administrations can be explained by differences in the underlying factor scores.

-   Residual invariance: The residual variance observed in the item scores, that can not be explained by the underlying factor, is equally large across all administrations.

The last level, the test for residual invariance is of largest interest, when discussing random measurement error or noise in the data. Residual variance describes the variation in scores for a single item (or several), that can not be explained by the latent factor. In the terminology of Classical Test Theory, this can be interpreted as error score variance (albeit there are some caveats, see ref XY). The test for residual invariance assesses whether these variances are identical across all groups. Thereby, Measurement Invariance Testing allows fora formal test, in how far the measuring quality in terms of noise is equivalent across samples.

Returning to our exemplary case, in @eq-RMA2 we observed that identical reliability coefficients might arise, even though measurement quality in terms of noise differed across samples. With some additional assumptions (unidimensionality, tau-equivalence, additive linear model - see reference), that we implicitly assume by averaging item responses to derive individual Extraversion scores, we can use the test for residual invariance to test for such differences in noise. If we observe a pattern like @eq-RMA2, where noise was larger in the market research panel, the test for residual invariance (if power was adequate) should indicate that the test does not display residual invariance across samples. This means the test highlights that there are differences in the noise (here referred to as residual variance). In a pattern like @eq-RMA2, where reliability coefficients differed but noise was identical in market research panel and psychology student sample, the test for residual invariance should demonstrate just that (given adequate power) - there are no differences in noise across administration sites.

The test for residual invariance is essentially tested just like the other levels of measurement invariance testing. A CFA-model with free item residual variances (meaning the item residual variance is estimated from the data) is tested for model fit against a model where the residual variances are constrained to be identical across samples. If the model with free residual variances fits substantially better than the model with constrained residual variances according to a number of test parameters and fit-indices, it indicates that the test does not exhibit residual invariance. Essentially, the hypothesis that the random variation in item scores is identical across all groups has to be rejected.

In practice, researchers evaluate this comparison using both chi-square difference testing and changes in approximate fit indices. A traditional approach is to conduct a chi-square difference test between the unconstrained and constrained models; a statistically significant $\Delta\chi^2\ (p < .05)$ suggests that constraining the residual variances leads to a meaningful loss of model fit, indicating a lack of residual invariance. However, because the chi-square test is highly sensitive to sample size, many researchers additionally rely on changes in practical fit indices.

Commonly examined indices include differences in the Comparative Fit Index ($\Delta CFI$), Root Mean Square Error of Approximation ($\Delta RMSEA$), and Standardized Root Mean Square Residual ($\Delta RSMR$). Small changes in these indices (e.g., $\Delta CFI \leq .1$, $\Delta RMSEA \leq .015$, $\Delta RSMR \leq .01$) are typically interpreted as evidence that the additional constraints do not substantially worsen model fit, thereby supporting residual invariance. By combining statistical significance testing with changes in fit indices, researchers obtain a more robust evaluation of whether residual invariance holds across groups.

If the test for residual invariance indicates that residual invariance *does not hold*, this implies that the residual variance for at least one item differs across groups. In other words, constraining the item-specific error variances to be equal significantly worsens model fit, suggesting that the amount of noise is not identical across groups. When we compute a mean score for Extraversion by averaging the observed item scores, we implicitly assign equal weight to each item. If residual variances differ substantially, this implies that some items contain more measurement error in certain groups, which challenges the assumption of equivalent measurement quality. Thus, testing residual invariance allows us to evaluate whether the amount of noise (i.e., random measurement error) is comparable across groups.

These tests for residual invariance can not inform us on how large the differences in noise (residual variances) are in practice. While it is technically possible to derive estimates regarding the size of residual variance across groups, these tests do not come with an effect size or measure of heterogeneity of how large the differences are across the samples. In our example, we assumed in noise or error score variance where 1 and 1.5 respectively for the online market research panel and the psychology student sample - an absolute difference of .5. However, whether the differences were .5 or 17, the test for residual invariance can not inform us about the magnitude of the difference, just whether there was a difference in the first place. 

Generally, tests for measurement invariance come with a number of other limitations, that are not central to this manuscript. Interested readers are referred to publication X & Y.

Measurement invariance testing and the test for residual invariance especially offer a model-based perspective on measurement quality. Essentially, the test for residual invariance allow researchers to test whether measurement quality is equivalent across groups by comparing the item-level residuals. Unlike Reliability Generalization Meta-Analysis, which treated measurement quality as the ratio of signal to total variance, a test for residual invariance can test for absolute differences in noise. The test for residual invariance does not offer a quantification of the differences in said noise. However, this question might be central to the question of differences in measuring quality - maybe we are not only interested in "Are there differences in measuring quality between the scores from the market research panel and the psychology student sample?" but also the question "How large are these differences?". For such questions, Boot-Err might be an appropriate answer, modelling noise explicitly by estimating and testing heterogeneity in error score variance.


### Boot-Err - inspecting differences in random measurement error

We developed Boot-Err to fill that gap. Boot-Err allows to explicitly model variance components such as error score variance, coming with a null hypothesis test for differences in error score variance and an estimate of heterogeneity, informing users about the extent of these differences. A detailed description on how to perform an analysis using Boot-Err can be found in section "Tutorial". We provide an in-depth description of the analytic formalisation and how it can be derived in Beinhauer et al. [-@beinhauer2025].

Discuss why measurement quality can be (inverse?) described as error variance.

To broadly outline the underlying idea: As discussed, a reliability coefficient quantifies the relative amount of random error variance in the sample's total score variance. This means that we can use both estimates to generate a third estimate - an estimate of error score variance as $\hat{\sigma}^2_E = \hat{\rho}_{xx'} \hat{\sigma}^2_X$, where $\hat{\rho}_{xx'}$ is the reliability coefficient and $\hat{\sigma}^2_X$ is the total score variance. Boot-Err then makes use of Bootstrapping techniques to estimate the standard error, and appropriate transformations so that the error score variance can be modelled using random-effects meta-analysis (including the appropriate back-transformation. These random-effects meta-analyses provide both a test on whether the differences in random error score variance are statistically significant, as well as an estimate of heterogeneity - *how* different is the error score variance across samples? Larger heterogeneity implying more variance in, and therefore larger differences in, error score variance.

Unlike analyses of reliability coefficients, error score variance quantifies the total amount of random measurement error in the data. Thereby, unlike Reliability Generalization Meta-Analysis, analysis employing Boot-Err is not affected by differences in true score variance. This means that, even if the underlying factor is not distributed identically across samples, Boot-Err can nonetheless identify differences in the extent of random measurement error. Similarly to measurement equivalence testing, Boot-Err provides a test for statistical significance of these differences. Beyond that, Boot-Err provides an estimate of the extent of these differences in the form of heterogeneity: it quantifies the variance in error score variance across administration sites.

For our example, we identified differences in reliability coefficients across different administration sites like the online market research panel and Psychology student sample. From Reliability Generalization Meta-Analysis we know that measurement quality, in terms of relative amounts of noise in the observations of Extraversion, is not identical across these administration sites. From the tests of measurement equivalence, we learned that there are differences in Extraversion measurement quality, in terms of total amounts of noise in the data, across the administration sites. Using Boot-Err we can also identify differences in the total amounts of noise, by inspecting error score variance. Beyond the results gained from measurement equivalence tests, Boot-Err provides an estimate of how strongly the measurement quality diverges across administration sites. Essentially, Boot-Err thereby estimates the extent of measurement (non-)invariance identified in the test for residual invariance.

## Tutorial

### Step 0 - Preparing Data

Since each data-set is different, depending on how data was collected and possibly aggregated, Step 0 is not formalized in the Boot-Err approach. Instead, the data needs to be cleaned and appropriately manipulated beforehand. By default, the Boot-Err approach makes use of list-wise deletion, if any observation for an item is missing. This tutorial makes use of data in a wide format. Concerning list-wise deletion, this implies that if a participant fails to respond to one or several items from the measuring instrument, all data from that participant is disregarded. Essentially, the entire row from the wide-format data-set is deleted/ignored. Researchers that prefer to make use of more sophisticated approaches to missing data, such as multiple imputation, will need to perform this within step 0.

For the purpose of this tutorial, we use the Boot-Err approach to assess in how far random error score variance is identical across administrations of the Conscientiousness dimension from the HEXACO personality inventory. In the Registered Report 10, Verschuere et al. [-@RRRMazar] replicated Experiment 1 from Mazar et al. (2008) across 19 labs. The original study tested the self-concept maintenance theory, which implies that people are willing to be dishonest about their performance in a task if it helps to maximize profits nut only as long as they can hold a positive view of themselves. In the original Experiment 1, participants were asked to solve a number of matrices in a problem-solving task. During the experiment, participants were briefly left alone, allowing them to claim they solved any arbitrary number of matrices. As participants were paid according to their performance in the task, both an opportunity and the incentive to cheat were induced. However, participants were randomly assigned to either a control group, where they recalled 10 books they read in high school, or a treatment group, where they recalled the Ten Commandments. Recalling the Ten Commandments was supposed to serve as a moral reminder, increasing the participants' attention to a moral standard of honesty. While recalling 10 books was a neutral task, recalling the Ten Commandments was supposed to remind participants that cheating for personal gain is "bad" and clashes with a positive self-image. Therefore, Mazar et al. expected participants in the treatment condition to report a lower number of solved matrices than in the control condition. In the original study, the HEXACO personality inventory was included as a filler task and for exploratory analyses regarding potential moderators of dishonesty. We chose the conscientiousness dimension of HEXACO for no specific reason, any of the other five subscales could have been used for the means of this tutorial.

```{r, output = F, warning = F}
# relevant libraries required for this script
packages <- c("magrittr", "dplyr", "osfr", "here", "zip", "data.table", "metafor", "boot")

# check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  pkg_avail <- nzchar(system.file(package = x))   # check if library is installed on system
  
  if(pkg_avail){
    require(x, character.only = TRUE)             # load the library, if already installed
    
  }else{
    install.packages(x)                           # install the library, if missing
    require(x, character.only = TRUE)             # load after installation
  }
})
```

```{r, warning = F, eval = F}
library(here)

dir.create(here("Data"), showWarnings = FALSE)
dir.create(here("Data/Downloaded Data"), showWarnings = FALSE)


# use osfr-package to download RRR10 data on the Mazar et al. / Srull et al. replications
osfr::osf_retrieve_file("https://osf.io/fwnc2") %>% 
  osfr::osf_download(path = here("Data/Downloaded Data/"), conflicts = "overwrite")
# zip-file is stored at Downloaded Data

# again, unzip the file and move it to the corresponding directory
unzip(here("Data/Downloaded Data/Meta-Analysis_2018-07-09.zip"),
      files = "Meta-Analysis/Results/raw_data_corrected_MAA.csv",
      exdir = here("Data/Original Data/RRR10"),
      junkpaths = T)
```

While the data cleaning and manipulation procedure is bound to be different for every research project, we briefly discuss how we come to a final data-set for this tutorial. Firstly, we download the zipped replication data-set from the RRR10 OSF-repository (osf.io/fwnc2), using the osfr R package version 0.2.9 (ref.). After unzipping, we can read the data-file in R. We want to restrict our analysis to only those participants, which were included in the replicators' primary analyses. Therein, we end up with 4,674 participants from 19 laboratories. We achieve this in substep *a)* by identifying the relevant laboratories from the published manuscript and filtering out all laboratories that do not belong to the primary replication.

```{r, warning = F}
# HEXACO Conscientiousness

# a) Load data and select laboratories part of primary replication

# Thankfully, for the RRR10 project, the data can be taken from a file directly, without the need of combination:
pc_df <- as.data.frame(fread(here("Data/Original Data/RRR10/raw_data_corrected_MAA.csv")))

# data <- data[which(data$age >= 18 & data$age <= 25),]
# retain observations only for participants eligible for analysis
pc_df <- pc_df[which(pc_df$inclusion == "inclusion both RRR" | pc_df$inclusion == "inclusion Mazar only"),] %>%
  mutate(source = lab.name)

# identify relevant labs for analsysis
labs_in_paper <- c("Laine", "klein Selle & Rozmann", "Aczel", "Ferreira-Santos", "Meijer", "Loschelder", "Wick", "Suchotzki", 
                   "Sutan", "Vanpaemel", "Verschuere", "Wiggins", "Gonzalez-Iraizoz", "Koppel", "Birt", "McCarthy", "Evans", 
                   "Holzmeister", "Ozdogru")
labs_in_data <- unique(pc_df$source)
labs_in_data[8] <- "Gonzalez-Iraizoz"
labs_in_data[16] <- "Ozdogru"

# remove labs from data, which we do not need for analysis
labs_excl <- labs_in_data[!labs_in_data %in% labs_in_paper]
pc_df <- pc_df[which(!pc_df$source %in% labs_excl),]

# # include only participants in cheat condition (design was 2x2, cheat - no cheat x commandment - books)
# pc_df <- pc_df[which(pc_df$maz.cheat.cond == "cheat"),]

```

The data-set contains a large number of information, that we do not need. Therefore, in substep *b)* we retain only those columns from the data-set, that contain information about the participants' responses to the HEXACO-items and in which laboratory the participant was assessed. Since some of the HEXACO-items are reverse-coded, we need to re-code half of the items, so the scale is identical across all items.

```{r, warning = F}
# b) Select relevant variables and re-code items that are coded in the opposite direction

# retain only those columns, which are needed for subsequent analysis.
pc_df <- pc_df[,c(which(names(pc_df) %in% c("source")),
                  grep("^hex", names(pc_df)))]

# recoding the hexaco items, that need recoding

for(i in grep("^hex", names(pc_df))){
  pc_df[,i] <- as.integer(pc_df[,i])
}
# these are the numbers of the items, that need recoding
items_hex_recode <- c(30, 12, 60, 42, 24, 28, 53, 35, 41, 59, 28, 52, 10, 46, 9, 15, 57, 21, 26, 32, 14, 20, 44, 56, 1, 31, 49, 19, 55, 48)
names_items_hex_recode <- paste0("hex", items_hex_recode) # pasting "hex" and number gives the column names

names_items_hex_recode_R <- paste0(names_items_hex_recode, "_R") # adding _R for names of items, that are recoded
pc_df[,names_items_hex_recode_R] <- 6 - pc_df[,names_items_hex_recode] # recode items that need recoding
```

Lastly, in substep *c)*, we remove all columns that contain responses on the other HEXACO-dimensions, which are not Conscientiousness. Therein, we have constructed a data-set, containing the responses on Conscientiousness from all participants that are part of the primary replication sample in RRR10.

```{r, warning = FALSE}

# c) select items belonging to Conscientiousness subscale

#Conscientiousness
items_hex_CO <- c(2, 26, 8, 32, 14, 38, 50, 20, 44, 56) # items in Conscientiousness subscale
names_items_hex_CO <- ifelse(items_hex_CO %in% items_hex_recode, paste0("hex", items_hex_CO, "_R"), paste0("hex",items_hex_CO)) # did item need recoding?
pc_hex_items_CO <- which(names(pc_df) %in% names_items_hex_CO) # select all items from Conscientiousness subscale, correctly coded

pc_df <- pc_df[,c(names_items_hex_CO, "source")]

# pc_df <- na.omit(pc_df)

labs <- unique(pc_df$source)

```

However, we find that some participants' responses on the Conscientiousness-items are missing in this data-set. While this was of no concern to the replication-authors, as the HEXACO-inventory served as a filler instrument, the same is not true for this tutorial. We will use Cronbach's Alpha to estimate score reliability. Cronbach's Alpha can not deal with missing responses. To keep it simple for this tutorial, we make use of list-wise deletion to remove any missing responses from the data-set. This means that the data for each participant, where any response on one or several items is missing is deleted from the data-set. Alternative approaches, that attempt to impute the missing responses while retaining the general covariance-structure of the data-set are available. Great examples include Multiple Imputation or Multivariate Imputation by Chained Equations. Singe imputation methods like Mean or Mode Imputation restrict the data-sets covariance structure, leading to estimation issues and biases in subsequent analyses.

```{r, warning = FALSE}
# d) dealing with missing responses

colSums(is.na(pc_df))

pc_df <- na.omit(pc_df)
```

Finishing step 0, we have constructed a data-set, containing only the responses of participants that belong to the replication authors' primary analysis sample, additionally removing missing responses from the data-set using listwise deletion. Subsequently, we can begin to follow the Boot-Err procedure.

### Step 1 - estimating score reliability

The first step in the Boot-Err procedure is to generate estimates of score reliability for each sample or administration site respectively. In the following code, we use an *sapply*-loop to compute Cronbach's Alpha separately for each laboratory in the data-set. Cronbach's Alpha is generally defined in the following @eq-alpha_formal (ref.).

$$\alpha=\frac{k^2\ \bar{\sigma}_{ij}}{\sigma^2_x}$$ {#eq-alpha_formal}

Here, $k$ corresponds to the number of items in the scale, $\bar{\sigma}_{ij}$ constitutes the average covariance between items and $\sigma^2_x$ constitutes the total score variance in the total test results.

However, computing the average item covariance can be computationally intensive, and a more efficient computation technique is available in @eq-alpha_fast (ref.).

$$\alpha = \frac{k}{k-1}\left(1-\frac{\sum_{i=1}^k\sigma^2_{i}}{\sum_{i=1}^k\sum_{j=1}^k\ \sigma_{ij}}\right) $$ {#eq-alpha_fast}

Here, $\sigma^2_{i}$ describes the variance of scores on item $i$, while $\sigma_{ij}$ describes the covariance between items $i$ and $j$. The equation in @eq-alpha_fast computes the same Cronbach's Alpha as in @eq-alpha_formal, but much more efficiently. By using *sapply* to quickly loop across all laboratories, we can efficiently generate a vector containing all estimates of Cronbach's Alpha for each administration site respectively.

```{r, warning = F}
alpha_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  C <- cov(dat)
  j <- dim(C)[1]
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  return(alpha)
  
})

alpha_vec
```

### Step 2 - estimating error score variance

After generating estimates of score reliability using Cronbach's Alpha, we can turn towards Step 2 of the Boot-Err procedure - estimating error score variance. According to CTT, error score variance is defined as described in @eq-error_var.

$$ {\sigma}^2_e = \left(1 - \rho_{xx'} \right) \sigma^2_x$$ {#eq-error_var}

This means, provided we generate an estimate of total score variance, we can use @eq-error_est to estimate error score variance, as we already have estimate of score reliability in the form of Cronbach's Alpha readily available.

$$ \hat{\sigma}^2_e = \left(1 - \hat{\alpha} \right) \hat{\sigma}^2_x$$ {#eq-error_est}

Here, $\hat{\alpha}$ describes the estimate of Cronbach's Alpha and $\hat{\sigma}^2_x$ constitutes an estimate of total score variance.

In the following code we, again, use an *sapply*-function to efficiently repeat the same computation for each laboratory. We quickly estimate the total score variance $\sigma^2_x$ by averaging the item scores separately for each participant (generating the test score) and computing the variance of these test scores. Since the *sapply*-function also returns a vector of $\sigma^2_x$-estimates, we can easily use @eq-error_est to generate a vector containing error score variance $\sigma^2_e$-estimates for each laboratory respectively.

```{r}

var_X_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  var_X <- dat %>% 
    rowMeans(.) %>% 
    var(.)
  
  return(var_X)
  
})

var_E_vec <-  (1-alpha_vec) * var_X_vec

var_E_vec

```

### Step 3 - construct Standard Errors using Bootstrapping

As described in the section *Analysis of error score variance - Boot-Err*, we use a random-effects meta-analysis to estimate and test for heterogeneity in error score variances. Since estimation of total score variance and its components is not variance-stable, we need to use a variance-stabilizing transformation. Bartlett and Kendall [-@bartlett1946] show that using the natural log, as in @eq-stable_var, is a suitable variance-stabilizing transformation for total score variance estimates. $$ T[\hat{\sigma}^2_x] = ln \left(\hat{\sigma}^2_x \right) $$ {#eq-stable_var}

While little evidence on variance-stabilizing transformations for score variance components is available, we have used the natural log for both the total and the error score variance component in our proposal for the Boot-Err [@beinhauer2025]. In the simulation scheme, we have not found a large bias in the estimates which would suggest that the transformation described in @eq-stable_var would be unsuitable for the variance component of error score variance $\sigma^2_e$. Accordingly, we are confident in our claim, that @eq-stable_varE describes a suitable variance-stabilizing transformation.

$$ T[\hat{\sigma}^2_e] = ln \left(\hat{\sigma}^2_e \right) $$ {#eq-stable_varE}

However, in order to run a random-effects meta-analysis, we not only need a variance-stable estimate, but also a Standard Error, quantifying our uncertainty in that estimate. While no analytic approximation of a Standard Error for log-transformed error score variance has been derived, we can use Bootstrapping techniques to generate robust Standard Error estimates for $T[\hat{\sigma}^2_e]$.

The package *boot* provides easy-to-use and efficient functions (including parallelization if desired) for Bootstrapping techniques [@boot]. In this tutorial, we make use of version 1.3-31. In order to use *boot*, we need to provide the package with a user-written function, which needs to do two things: i) allow the *boot*-package to sample from our data-set (we do so by `dat_boot <- data[indices,]`), and ii) compute the parameter on the re-sampled data-set that we want to boot-strap. In the following code-chunk, we show a function that does just that.

```{r}

ln_var_E_boot_func <- function(data, indices){
  
  dat_boot <- data[indices,]
  
  C <- cov(dat_boot)
  j <- dim(C)[1]
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  var_E <- var_X * (1 - alpha)
  
  ln_var_E <- log(var_E)
  
  return(ln_var_E)
  
}
```

Now that we have a function available for *boot* to re-sample our data and estimate $T[\hat{\sigma}^2_e]$ in the bootstrapped samples, we need to supply our user-defined function to *boot* and compute the Standard Error from the bootstrapped estimates. Essentially, what we do here is estimate the Standard Error of log-transformed error score variance, by computing the standard deviation of all bootstrapped estimates for a single laboratory, see @eq-bootSE.

$$ SE\left[T[\hat{\sigma}^2_e]\right] = \sqrt{\frac{1}{n}\sum_{i=1}^n(T[\hat{\sigma}^2_e]_i - \bar{T}[\hat{\sigma}^2_e])^2}$$ {#eq-bootSE}

Here, $n$ describes the number of bootstrap-samples drawn, whereas $\bar{T}[\hat{\sigma}^2_e]$ describes the mean of the log-transformed error score variance estimates of the $n$ bootstrap samples. In the following code-chunk, we use the *boot* package to estimate $SE\left[T[\hat{\sigma}^2_e]\right]$ from 3000 bootstrap samples for each laboratory respectively. Again, we make use of an *sapply*-function to loop efficiently across laboratories.

```{r, warning = FALSE}


ln_var_E_SE_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_E_boot_func,
                               R = 3000)
  
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})
```

Lastly, we still need to formally transform and store the individual estimates of $T[\hat{\sigma}^2_e]$ for each laboratory (so far we only did this for the bootstrap-samples). However, at this point, computing this transformation is trivial. To make things easier for us in the following steps, we combine the estimates of log-transformed error score variance and its Standard Errors here in a single data-frame. We add the laboratory names to the data-frame, so estimates can be linked to their respective sources.

```{r, warning = FALSE}
ln_var_E_vec <- log(var_E_vec)

ln_var_E_df <- data.frame(ln_var_E = ln_var_E_vec, 
                          ln_var_E_SE = ln_var_E_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

ln_var_E_df
```

Concluding step 3, we have gathered all estimates we need, to perform a random-effects meta-analysis on (log-transformed) error score variances.

### Step 4 - meta-analysis of error score variance

For readers who only wish to use Boot-Err, Step 4 is very simple \[Ich habe mal drüber nachgedacht hier als zusätzlichen input das random-effects model zu erklären - der Part hier ist aber doch etwas länger geworden, braucht's das in dem Sinne denn überhaupt?\]. We use the R-package *metafor*, as it provides an efficient and easy-to-use implementation of random-effects meta-analyses for any parameter [@metafor]. Using the *rma*-function, with the argument *method = "REML"*, ensures that we use the REML-estimator of heterogeneity. Hönekopp and Linden [-@hoenekopp2022] have shown that the REML-estimator tends to outperform other estimators across a wide range of conditions. Therefore, the following chunk will run a random-effects meta-analysis in *metafor*, generating an estimate of the meta-analytic mean (in the output `estimate`), estimates of absolute heterogeneity in standard deviation and variance (in the output `tau` and `tau^2`) and estimates of relative heterogeneity in the form of $I^2$ and $H^2$ (in the output `I^2` and `H^2`). Additionally, *metafor* provides the results for two hypothesis tests. Firstly, the test for heterogeneity (`Test for Heterogeneity` in the output), assessing whether the estimate of heteorgeneity $\tau^2$ can be statistically distinguished from $0$. Secondly, the test for the meta-analytic effect size, assessing whether the meta-analytic aggregate can be statistically distinguished from $0$ (in the Output `Model results`).

```{r}

ln_var_E_rma <- metafor::rma(data = ln_var_E_df,
                             yi = ln_var_E,
                             sei = ln_var_E_SE,
                             method = "REML")

ln_var_E_rma

```

The significance tests indicate two things for the log-transformed error score variance of Conscientiousness across replications in Verschuere et al. [-@RRRMazar]: Firstly, we find that the meta-analytic estimate of `r paste0("$\\mu_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$b[1], 3), "$")` is significantly different from zero (`r paste0("$z = ", round(ln_var_E_rma$zval, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$pval < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$pval, 3))), "$")`), using a significance level of 5%. However, this is not meaningful information, as the estimate is on the log-transformed scale and can not be interpreted. Secondly and more interestingly, we find that the test for heterogeneity indicates that there is statistically significant heterogeneity in log-transformed error score variances (`r paste0("$Q(", ln_var_E_rma$k - 1, ") = ", round(ln_var_E_rma$QE, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$QEp < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$QEp, 3))), "$")`). Even though the tested estimates are still on the log-scale, we can nonetheless infer from this result, that error score variance $\sigma^2_e$ is not identical across all administrations of the Conscientiousness-scale. However, the estimate of `r paste0("$\\tau^2_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$tau2, 3), "$")` is uninterpretable due to the log-transformation.

The estimates of relative heterogeneity can be interpreted despite the transformation. The estimate of `r paste0("$I^2 = ", round(ln_var_E_rma$I2, 2), "$")` implies that `r round(ln_var_E_rma$I2, 2)`% of the variance observed in the estimate of error score variance are due to actual heterogeneity in the population, not due to sampling error. Similarly, the estimate of `r paste0("$H^2 = ", round(ln_var_E_rma$H2, 2), "$")` implies that the heterogeneity is `r round(ln_var_E_rma$H2, 2)` times as large as the variance induced by sampling error. According to standard conventions (ref.), both estimate imply a strong or large extent of heterogeneity. However, both estimates of relative heterogeneity are often criticized, as they depend strongly on sample size and are easily manipulated (refs.). Instead, we should turn towards an informative interpretation of the absolute heterogeneity in terms of $\tau^2$ or $\tau$.

To do so, we need to back-transform our newly generated estimates to the original scale. This is done in Step 5.

### Step 5 - Backtransforming estimates

The transformation of score variance (components) using the natural log not only serves to generate variance-stable estimates. Additionally, it comes with the added benefit, that we can assume $T[\sigma^2_e]$ to follow a normal distribution. Several authors have proposed that the total score variance, if not constant, can be described by a log-normal distribution (refs.). This means that the log-transformed estimates accordingly follow a normal distribution. This is very neat for the application of the Boot-Err technique, as one central assumption of a random-effects meta-analysis is that the parameter follows a normal distribution. Additionally, it helps us back-transform the estimates of meta-analytic mean $\mu_{T[\sigma^2_e]}$ and its heterogeneity $\tau^2_{T[\sigma^2_e]}$, as equations are readily available (refs.).

$$ \hat{\mu}_{\sigma^2_e} = \exp\left(\hat{\mu}_{T[\sigma^2_e]} + \frac{1}{2} \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_mu}

@eq-backtransf_mu demonstrates how the mean error score variance $\hat{\mu}_{\sigma^2_e}$ can be estimated from the meta-analytic estimates of $T[\sigma^2_e]$. Here, $\hat{\mu}_{T[\sigma^2_e]}$ represents the estimate of meta-analytic mean of log-transformed error score variance and $\hat{\tau}^2_{T[\sigma^2_e]}$ describes the estimate of its heterogeneity. Similarly, @eq-backtransf_var can be used to derive the heterogeneity of error score variance $\hat{\tau}^2_{\sigma^2_e}$ on its original scale, from the same meta-analytic estimates of \$T\[\sigma\^2_e\]\$\$.

$$ \hat{\tau}^2_{\sigma^2_e} = \exp\left(\hat{\tau}^2_{T[\sigma^2_e]} - 1\right) \cdot \exp\left(2 \hat{\mu}_{T[\sigma^2_e]} + \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_var}

In the following chunk, we define two functions which compute [Equations -@eq-backtransf_mu] and [-@eq-backtransf_var] from the *metafor*-output. The functions only take the *metafor*-object generated from the *rma*-function as input and return the estimates $\hat{\mu}_{\sigma^2_e}$ and $\hat{\tau}^2_{\sigma^2_e}$ respectively.

```{r}
backtransform_var_mu <- function(rma_obj){
  exp(rma_obj$b[1] + (.5*rma_obj$tau2))
}

backtransform_var_tau2 <- function(rma_obj){
  (exp(rma_obj$tau2) - 1) * exp((2 * rma_obj$b[1]) + rma_obj$tau2)
}

backtransform_var_mu(ln_var_E_rma)

backtransform_var_tau2(ln_var_E_rma) %>% 
  sqrt(.)
```

Running the two functions in the chunk above returns two estimates: The meta-analytic mean error score variance of the HEXACO-Conscientiousness scale across 19 laboratories is $\hat{\mu}_{\sigma^2_e} = `{r} round(backtransform_var_mu(ln_var_E_rma), 3)`$; The estimate for its heterogeneity is $\hat{\tau}_{\sigma^2_e} =`{r} round((backtransform_var_tau2(ln_var_E_rma) %>%   sqrt(.)), 3)`$. To understand what this implies, we can assess the relative size of heterogeneity, also known as the coefficient of variation, which is computed as $CV = \frac{\tau}{\mu}$. Here we find a CV of $CV_{\sigma^2_e} = `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3)`$. This means that the extent of the differences in error score variances is approximately `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance found across these laboratories.

None of the other approaches, such as RG-MA, IRT or measurement invariance testing allow to generate these estimates we have derived using the Boot-Err technique. For researchers who are more accustomed to these scales, these estimates can be highly valuable to understand how much variance in scores we can expect to be introduced by measurement imprecision across administrations of the same scale. Since we are no experts in use of the Conscientiousness-scale, we have to look slightly further beyond. However, as described in the following Bonus-Step, we can follow the Boot-Err technique with an additional parameter, generating much more informative insights.

### Bonus-Step - Relating error score variance to total score variance

Score reliability $\rho_{xx'}$ is defined as the relative extent to which the score variance observed is attributable to actual differences in the true scores, not to measurement imprecision. However, that also means that $1 - \rho_{xx'}$ describes the relative extent to which the score variance is attributable to random measurement error.

Similarly, on the meta-analytic scale, we can look at the heterogeneity in total score variance $\sigma^2_x$, and attempt to understand how much of its heterogeneity came to be through differences in error score variance. Intuitively, many researchers tend to ascribe heterogeneity in total score variance $\sigma^2_x$ to differences in measurement precision. This means that often, a larger score variance is interpreted as if it came to be through larger measurement error such as $\sigma^2_e$. However, obviously parts of the heterogeneity might also be better explained by differences in the true score variance $\sigma^2_t$. Fortunately, if we follow Steps 3 through 5 for the total score variance $\sigma^2_x$ instead, we can assess in how far its heterogeneity can be attributed to differences in true score variance $\sigma^2_t$ or measurement error $\sigma^2_e$.

In the following chunk, we briefly run through these steps to estimate heterogeneity in total score variance $\sigma^2_x$.

```{r}
ln_var_X_boot_func <- function(data, indices){
  
  dat_boot <- data[indices,]
  
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  ln_var_X <- log(var_X)
  
  return(ln_var_X)
}


ln_var_X_SE_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_X_boot_func,
                               R = 3000)
  
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})

ln_var_X_vec <- log(var_X_vec)

ln_var_X_df <- data.frame(ln_var_X = ln_var_X_vec, 
                          ln_var_X_SE = ln_var_X_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

ln_var_X_rma <- metafor::rma(data = ln_var_X_df,
                             yi = ln_var_X,
                             sei = ln_var_X_SE,
                             method = "REML")


backtransform_var_mu(ln_var_X_rma)

backtransform_var_tau2(ln_var_X_rma) %>% 
  sqrt(.)

```

We find a meta-analytic mean of total score variance $\hat{\mu}_{\sigma^2_x} = `{r} round(backtransform_var_mu(ln_var_X_rma), 3)`$ and a heterogeneity of $\hat{\tau}_{\sigma^2_x} = `{r} round((backtransform_var_tau2(ln_var_X_rma) %>% sqrt(.)), 3)`$. Under CTT, total score variance is defined as the sum of true and error score variance $\sigma^2_x = \sigma^2_e + \sigma^2_t$, which are assumed to be independent. This means that we can assess how much heterogeneity in total score variance can actually be explained by differences in measurement error:

$$ R = \frac{\hat{\tau}^2_{\sigma^2_e}}{\hat{\tau}^2_{\sigma^2_x}} $$ {#eq-ratio}

In @eq-ratio, we define the ratio-variable $R$ as the ratio of $\hat{\tau}_{\sigma^2_e}$ to $\hat{\tau}_{\sigma^2_e}$. For Conscientiousness, we find a ratio of $R  = `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3)`$. This means that about `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3) * 100`% of heterogeneity in $\sigma^2_x$ could be explained by differences in random measurement error. This also means that `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`% of this heterogeneity needs to be explained by differences in true score variance $\sigma^2_t$.

### Conclusion

Using the Boot-Err technique, we can use random-effects meta-analysis and appropriate variance-stabilizing transformations paired with Bootstrapping techniques to assess in how far there is heterogeneity in random measurement error across administrations of the same scale. Making use of the data supplied by Verschuere et al. [-@RRRMazar], we used Boot-Err to understand in how far there are differences in absolute measurement precision across 19 laboratories where the HEXACO-Conscientiousness personality scale was administered. We not only find statistically significant differences in error score variance, but were also able to estimate the absolute and relative extent of these differences. Therein, we are certain that the random measurement error in the Conscientiousness scores is not constant across samples. We find that the heterogeneity is about `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance. Across administrations of the same scale, error score variance easily varies between `r round(exp(ln_var_E_rma$b[1] - sqrt(ln_var_E_rma$tau2)), 3)` and `r round(exp(ln_var_E_rma$b[1] + sqrt(ln_var_E_rma$tau2)), 3)` (mean value plus/minus heterogeneity, then back-transformed) \[add this to Bonus-Step?\].

In order to better understand what these differences mean, we additionally uncovered in how far the heterogeneity in total score variance can be attributed to these differences in measurement error. Here, we found that the overwhelming majority of heterogeneity, `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`%, can not be explained by differences in measurement error. For the Conscientiousness-scale, this implies that, while there clearly are substantial differences in measurement error across groups, the distribution of true scores varies much more strongly across groups. This is important, as this implies that the differences in total score variance can not be attributed to differences in measurement error. Instead, Conscientiousness is distributed highly different across those groups. In Beinhauer et al. [-@beinhauer2025liable] we discuss potential causes and implications of this phenomenon.

## Discussion

Measurement precision and how strongly it diverges across administrations of the same instrument is a central question in social and behavioural sciences. Differences in random measurement error are not only interesting to researchers assessing the quality of a measuring instrument. It also has clear implications when it comes to hypothesis testing and standardization of effect sizes. Therein, the assessment of measurement equivalence has implications for replication studies and meta-analytic applications. Across the first pages, we have summarised the most popular techniques to assess differences in random measurement error - Reliability Generalization Meta-Analysis, Measurement Invariance Testing and approaches stemming from Item Response Theory. Additionally, we introduced Boot-Err, a technique we developed to step away from relative measures such as score reliability or tests without estimates of how large the differences are, like Measurement Invariance Tests [@beinhauer2025].

In this tutorial, we have demonstrated how the Boot-Err can be used to generate detailed estimates and tests regarding the extent of the differences we observe in random measurement error, and how strongly they affect the differences in score variance. It is important to note that this technique comes with a number of limitations and constraints to generalizability. Just like for any statistical analysis, a number of assumptions need to be met. Regarding application of the Boot-Err, total and error score variance should follow a log-normal distribution and be computed from samples that are generally independent of each other (the individual samples should not be described as if stemming from a nested structure). Additionally, a number of assumptions relate to the type of score reliability estimate used. Here, we made use of Cronbach's Alpha, which typically comes with assumptions of tau-equivalence and unidimensionality (ref.). While these assumptions are widely criticized and rarely hold up in real data (ref.), we need to remind ourselves that these assumptions also relate to typical scoring methods of these measuring instrument. The HEXACO-personality score for each dimension is computed by taking the average of the relevant item scores \[check to make sure this is correct\]. Therein, even stricter assumptions of tau-equivalence and unidimensionality apply - making Cronbach's Alpha the more appropriate choice of score reliability estimate over other ones that may impose less strict assumptions.

Even though differences in absolute measurement precision are of central interest to measuring instrument and scale development, methods to assess differences in error score variance each come with their own caveats and shortcomings. Models regarding differences in score reliability fail to take differences introduced by true score variance into account, while models simply testing for differences in error score variance without actually estimating it fail to put these differences into perspective. Perhaps this is the reason why discussions surrounding the impact of random measurement error on standardized effect sizes and hypothesis tests has been overemphasized (refs. \[NEED TO PICK THIS UP IN INTRO\]), implying that removing random measurement error was akin to harmonizing variability across populations and data-sets.

Boot-Err, mends both these shortcomings, as differences in error score variance can be estimated and tested, but also put into perspective on how much heterogeneity in total score variance they actually produce. As we demonstrated in the Bonus-Step section, differences in error score variance can not explain why total score variance differs across groups--instead, differences in true score variance must come at a much larger extent. This implies that differences in score variability can not sufficiently be mended by taking care of measurement error. Instead, future research should ensure that the true score variance is stable across groups and representative for the respective population.

#### Abstellgleis

DAS HIER KÖNNT IHR IGNORIEREN - DAS SIND ALTE TEXTBAUSTEINE DIE FÜR DEN MOMENT KEINEN PLATZ HABEN

One central assumption of IRT is that the item characteristics are valid for all subgroups in the population to which the model is applied. This means that, no matter the sub-group the individual is from, the item-response-function should be identical. This means that all individuals with the same latent ability score $\theta$ have the same probability of getting an item right or wrong. As this is a central assumption of these models, routine procedures for examining differential item functioning (DIF) have been established. Typically, a *reference sample* should be available, where an IRT-scoring model has been established. Subsequently, we can assess if any items exhibit DIF in a *focal group*, where the same $\theta$ would lead to different observed scores.

## References

::: {#refs}
:::
