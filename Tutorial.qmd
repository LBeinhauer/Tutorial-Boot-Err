---
title: "Tutorial"
format: html
bibliography: bibliography/Boot-Err_Tutorial.bib
csl: bibliography/apa-6th-edition.csl
---

# New

Intro - initial question / that is best answered using Boot-Err

Notes Brainstorming Jens:

-   was kann RGMA nicht? Prädiktoren hinzufügen!

    -   Shaw etc! Prädiktoren

Perspektive:

Messinstrument?

Standardisierte Effekte?

-   Forschungsfrage pro Methode vs. unterschiedliche Definition pro Methode

-   Wenn Rel immer gleich, aber varianzen unterschiedlich, dann muss das Heteroskedastizität implizieren?

-   partial pooling um schätzungen der "wahren" Fehlervarianzen zu generieren

Jens Fünderich, 2026: Punktschätzungen der wahren Fehlervarianz

## Introduction

Researchers frequently seek to assess the consistency and accuracy of psychological measurements across different administrations of the same scale. The same scale administered to a different group of participants may not lead to scores of equivalent quality.

For instance, a psychological test measuring Extraversion might be administered to a group of participants in a German sample of Psychology students and a random sample from an international online panel for market research. Understanding whether the instrument produces results of a similar quality across different administration contexts is central to evaluating the generalizability of the instrument.

Generally, we assume that Extraversion is a psychological construct that describes a person's central characteristic, that we can observe in most populations around the world. Accordingly, we would assume, that we are also able to measure that characteristic in those populations using the same (likely translated) instrument. However, whether that instrument produces scores of the same measurement quality, is less obvious.

Throughout this text, when discussing measurement quality, we are referring to random measurement error. Unlike systematic measurement error, that describes systematic errors in measurement, biasing observed scores in specific directions, measurement error refers to the general fuzziness of inaccurate measures. While large systematic measurement error implies, for example, that measured scores are higher than they should be, large random measurement error implies that the measured scores randomly vary around the value they should take on. However, on average, the measurements should still be on point.

In order to properly interpret the scores derived from our Extraversion measuring instrument , we need to know whether the scores are of sufficient quality. And if we use the same instrument across several settings, in order to compare results across samples, we need to know if the measuring quality is equivalent. If, for example, the measuring quality is much lower in scores derived from our online panel, compared to the german psychology students, then we need to be cautious whether the observations can still be meaningfully interpreted. Similarly, we need to understand why our measuring instrument, can not capture a construct that we assumed to be universal equivalently well.

However, as we will see throughout this text, a definition of measurement quality describing random measurement error is fuzzy and can be interpreted in different ways. These different interpretations can lead to widely diverging conclusions on whether the random measurement error is equally large.

### Reliability generalization meta-analysis

A simple and accessible way to compare measuring quality across administrations of the same scale is by comparing the reliability coefficients computed for each administration sample. Reliability coefficients, such as Cronbach's Alpha, McDonald's Omega or Guttmann's Lambda IV attempt to quantify the amount of random measurement error\[ or noise\] present in the scors. These coefficients estimate and quantify the relative amount of noise in the observed scores' variance. Essentially, they describe how much of the observed score variance can be attributed to random measurement error. Differences in reliability coefficients therefore highlight, that the proportion of noise to signal in the data differs across administration sites.

Generally, reliability coefficients can take on values between 0 and 1, where 0 indicates that the variance in observations is made up of 100 % noise, while a 1 indicates that there is no noise in the data. Coming back to our example, assume we find a Cronbach's alpha .7 for the scores obtained from our sample of Psychology students, but a Cronbach's alpha of .9 for observations stemming from our online panel. Comparing these numbers, we can conclude that the proportion of noise to data is worse for data obtained from Psychology students than for data obtained from the online panel. Of the total score variance observed in each individual sample, a higher proportion can be attributed to true differences in Extraversion for the online panel, than for the student sample.

However, coefficients derived from samples are always prone to sampling error. We are aware of this truth everytime we make use of a test for statistical significance of a mean value or standardised effect size (or estimate a posterior distribution for mean value / effect size). Just like the population mean value is not necessarily identical to the estimate obtained from the sample, the same is true for the estimated reliability coefficient. Due to the randomness in the sample process, the estimated mean value or reliability coefficient may vary randomly around the population value.

This means that we can not simply compare the estimates of reliability coefficients, to determine whether there are true differences in measurement precision across administration sites. However, a formal method to assess whether reliability coefficients of the same instrument differ across administrations has been developed in the form of reliability generalization meta-analysis. Comparing and combining estimates from several studies or samples to derive appropriate mean values and \[estimate\] how strongly these estimates diverge is known as research synthesis. Techniques in the space of research synthesis typically attempt to model the sampling error introduced by the randomness of the sampling procedure, and correct for it when testing statistical significance or generating estimates. Therein, meta-analysis is a technique that incorporates the uncertainty of sample estimates and allows to test for and model the variance that goes beyond the sampling error. This variance, referred to as heterogeneity, describes the variation of the coefficient that we would have observed, if there was no sampling error.

Reliability Generalization Meta-Analysis aggregates estimates of the same reliability coefficient such as Cronbach's Alpha across several administrations of the same scale, to test whether there are statistically significant differences between these coefficients. Additionally, an estimate of heterogeneity can be derived, which describes how strongly the reliability coefficients vary. In our example case, we can compute Cronbach's Alpha for our Psychology student sample, for our market research panel and for the other administrations of the scale, that we want to compare. The Reliability Generalization Meta-Analysis will then tell us, whether the differences in Cronbach's Alpha are indeed statistically significant. Thereby, we can properly test whether measurement precision differs across administrations - in terms of how much noise is in the scores, proportional to their total variation.

That is because score reliability is actually the ratio of error score variance to total score variance, where the total score variance is made up of the sum of the error and the true score variance. True score variance essentially describes in how far the construct actually varies within the sample, undistorted by measurement error. In our example, for the online panel, the true score variance would describe the differences in the actual Extraversion across the participants in the sample. The error score variance on the other hand, describes the amount of random noise that is found in the observed scores, that can actually not be attributed to the actual underlying Extraversion. Therein, reliability coefficients can not inform us about the actual size of the random measurement error across administrations, or how well the items capture the underlying structure of Extraversion. From Reliability Generalization Meta-Analysis, we can only learn in how far the ratio of error score variance to total score variance is identical across administrations.

### Measurement invariance testing

Alternative techniques have been developed a long time ago, which allow to further assess in how far the measuring instrument addresses an underlying construct. Measurement invariance testing does just that, modelling and assessing in how far the measuring instrument captures the same underlying trait across several administrations of the same scale.

Measurement invariance describes a facet of the measuring instrument: that it is *invariant* in its measuring quality when administered to different groups. Different levels of measurement invariance exist, and in a series of four (sometimes more) steps, different levels are tested. The four levels can be summarised as:

-   Configural invariance: Across all administrations, the measuring instrument assesses the same number of factors. If several factors are assessed, the individual items relate to the same factors across all administrations.

-   Metric invariance: The factor loadings for each individual item are identical across all administrations.

-   Scalar invariance: Systematic differences in the observed scores between administrations can be explained by differences in the underlying factor scores.

-   Residual invariance: The residual variance observed in the item scores, that can not be explained by the underlying factor, is equally large across all administrations.

For the sake of this tutorial, we will not delve deeper into what each measurement invariance level entails and how it is tested in practice. Interested readers are referred to: publication 1, publication 2, 100%-CI blog. Instead, for this tutorial, we have limited our discussion to measuring quality as random measurement error or random noise. Broadly speaking, the first three levels of measurement invariance could be grouped as pertaining systematic error. They discuss differences introduced in the observed scores introduced by systematic differences in the relation between underlying factor and item score. Only the last level discusses random measurement error introduced by imprecise measurement.

A statistical test for residual invariances tests, whether the random noise observed in the item responses, that can not be explained by the underlying factor, is equally large. If the random variation in items scores for our online market research panel is larger than that of our Psychology student sample, the test will flag that the scores are not *residually invariant* (given the statistical power is appropriate, for discussions see publication 3). This implies that, on average, the estimate for Extraversion in our online panel will be farther of from the underlying true value, than the estimate obtained for our Psychology Students. Thereby, measurement invariance testing allows for a more nuanced discussion on whether the measuring instrument produces scores of equal quality across administrations.

However, measurement invariance testing does not allow for a quantification of how large the differences in measurement error are across the different administration sites. Measurement invariance testing only comes with a test, inspecting whether or not there are differences. There is no estimate or effect size that allows a researcher to assess in how far these differences are meaningful to subsequent interpretation.

### Boot-Err - inspecting differences in random measurement error

We developed Boot-Err to fill that gap. Boot-Err allows to explicitly model variance components such as error score variance, coming with a null hypothesis test for differences in error score variance and an estimate of heterogeneity, informing users about the extent of these differences. A detailed description on how to perform an analysis using Boot-Err can be found in section "Tutorial". We provide an in-depth description of the analytic formalisation and how it can be derived in Beinhauer et al. [-@beinhauer2025].

Discuss why measurement quality can be (inverse?) described as error variance.

To broadly outline the underlying idea: As discussed, a reliability coefficient quantifies the relative amount of random error variance in the sample's total score variance. This means that we can use both estimates to generate a third estimate - an estimate of error score variance as $\hat{\sigma}^2_E = \hat{\rho}_{xx'} \hat{\sigma}^2_X$, where $\hat{\rho}_{xx'}$ is the reliability coefficient and $\hat{\sigma}^2_X$ is the total score variance. Boot-Err then makes use of Bootstrapping techniques to estimate the standard error, and appropriate transformations so that the error score variance can be modelled using random-effects meta-analysis (including the appropriate back-transformation. These random-effects meta-analyses provide both a test on whether the differences in random error score variance are statistically significant, as well as an estimate of heterogeneity - *how* different is the error score variance across samples? Larger heterogeneity implying more variance in, and therefore larger differences in, error score variance.

Unlike analyses of reliability coefficients, error score variance quantifies the total amount of random measurement error in the data. Thereby, unlike Reliability Generalization Meta-Analysis, analysis employing Boot-Err is not affected by differences in true score variance. This means that, even if the underlying factor is not distributed identically across samples, Boot-Err can nonetheless identify differences in the extent of random measurement error. Similarly to measurement equivalence testing, Boot-Err provides a test for statistical significance of these differences. Beyond that, Boot-Err provides an estimate of the extent of these differences in the form of heterogeneity: it quantifies the variance in error score variance across administration sites.

For our example, we identified differences in reliability coefficients across different administration sites like the online market research panel and Psychology student sample. From Reliability Generalization Meta-Analysis we know that measuring quality, in terms of relative amounts of noise in the observations of Extraversion, is not identical across these administration sites. From the tests of measurement equivalence, we learned that there are differences in Extraversion measuring quality, in terms of total amounts of noise in the data, across the administration sites. Using Boot-Err we can also identify differences in the total amounts of noise, by inspecting error score variance. Beyond the results gained from measurement equivalence tests, Boot-Err provides an estimate of how strongly the measurement quality diverges across administration sites. Essentially, Boot-Err thereby estimates the extent of measurement (non-)invariance identified in the test for residual invariance.

## Tutorial

### Step 0 - Preparing Data

Since each data-set is different, depending on how data was collected and possibly aggregated, Step 0 is not formalized in the Boot-Err approach. Instead, the data needs to be cleaned and appropriately manipulated beforehand. By default, the Boot-Err approach makes use of list-wise deletion, if any observation for an item is missing. This tutorial makes use of data in a wide format. Concerning list-wise deletion, this implies that if a participant fails to respond to one or several items from the measuring instrument, all data from that participant is disregarded. Essentially, the entire row from the wide-format data-set is deleted/ignored. Researchers that prefer to make use of more sophisticated approaches to missing data, such as multiple imputation, will need to perform this within step 0.

For the purpose of this tutorial, we use the Boot-Err approach to assess in how far random error score variance is identical across administrations of the Conscientiousness dimension from the HEXACO personality inventory. In the Registered Report 10, Verschuere et al. [-@RRRMazar] replicated Experiment 1 from Mazar et al. (2008) across 19 labs. The original study tested the self-concept maintenance theory, which implies that people are willing to be dishonest about their performance in a task if it helps to maximize profits nut only as long as they can hold a positive view of themselves. In the original Experiment 1, participants were asked to solve a number of matrices in a problem-solving task. During the experiment, participants were briefly left alone, allowing them to claim they solved any arbitrary number of matrices. As participants were paid according to their performance in the task, both an opportunity and the incentive to cheat were induced. However, participants were randomly assigned to either a control group, where they recalled 10 books they read in high school, or a treatment group, where they recalled the Ten Commandments. Recalling the Ten Commandments was supposed to serve as a moral reminder, increasing the participants' attention to a moral standard of honesty. While recalling 10 books was a neutral task, recalling the Ten Commandments was supposed to remind participants that cheating for personal gain is "bad" and clashes with a positive self-image. Therefore, Mazar et al. expected participants in the treatment condition to report a lower number of solved matrices than in the control condition. In the original study, the HEXACO personality inventory was included as a filler task and for exploratory analyses regarding potential moderators of dishonesty. We chose the conscientiousness dimension of HEXACO for no specific reason, any of the other five subscales could have been used for the means of this tutorial.

```{r, output = F, warning = F}
# relevant libraries required for this script
packages <- c("magrittr", "dplyr", "osfr", "here", "zip", "data.table", "metafor", "boot")

# check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  pkg_avail <- nzchar(system.file(package = x))   # check if library is installed on system
  
  if(pkg_avail){
    require(x, character.only = TRUE)             # load the library, if already installed
    
  }else{
    install.packages(x)                           # install the library, if missing
    require(x, character.only = TRUE)             # load after installation
  }
})
```

```{r, warning = F, eval = F}
library(here)

dir.create(here("Data"), showWarnings = FALSE)
dir.create(here("Data/Downloaded Data"), showWarnings = FALSE)


# use osfr-package to download RRR10 data on the Mazar et al. / Srull et al. replications
osfr::osf_retrieve_file("https://osf.io/fwnc2") %>% 
  osfr::osf_download(path = here("Data/Downloaded Data/"), conflicts = "overwrite")
# zip-file is stored at Downloaded Data

# again, unzip the file and move it to the corresponding directory
unzip(here("Data/Downloaded Data/Meta-Analysis_2018-07-09.zip"),
      files = "Meta-Analysis/Results/raw_data_corrected_MAA.csv",
      exdir = here("Data/Original Data/RRR10"),
      junkpaths = T)
```

While the data cleaning and manipulation procedure is bound to be different for every research project, we briefly discuss how we come to a final data-set for this tutorial. Firstly, we download the zipped replication data-set from the RRR10 OSF-repository (osf.io/fwnc2), using the osfr R package version 0.2.9 (ref.). After unzipping, we can read the data-file in R. We want to restrict our analysis to only those participants, which were included in the replicators' primary analyses. Therein, we end up with 4,674 participants from 19 laboratories. We achieve this in substep *a)* by identifying the relevant laboratories from the published manuscript and filtering out all laboratories that do not belong to the primary replication.

```{r, warning = F}
# HEXACO Conscientiousness

# a) Load data and select laboratories part of primary replication

# Thankfully, for the RRR10 project, the data can be taken from a file directly, without the need of combination:
pc_df <- as.data.frame(fread(here("Data/Original Data/RRR10/raw_data_corrected_MAA.csv")))

# data <- data[which(data$age >= 18 & data$age <= 25),]
# retain observations only for participants eligible for analysis
pc_df <- pc_df[which(pc_df$inclusion == "inclusion both RRR" | pc_df$inclusion == "inclusion Mazar only"),] %>%
  mutate(source = lab.name)

# identify relevant labs for analsysis
labs_in_paper <- c("Laine", "klein Selle & Rozmann", "Aczel", "Ferreira-Santos", "Meijer", "Loschelder", "Wick", "Suchotzki", 
                   "Sutan", "Vanpaemel", "Verschuere", "Wiggins", "Gonzalez-Iraizoz", "Koppel", "Birt", "McCarthy", "Evans", 
                   "Holzmeister", "Ozdogru")
labs_in_data <- unique(pc_df$source)
labs_in_data[8] <- "Gonzalez-Iraizoz"
labs_in_data[16] <- "Ozdogru"

# remove labs from data, which we do not need for analysis
labs_excl <- labs_in_data[!labs_in_data %in% labs_in_paper]
pc_df <- pc_df[which(!pc_df$source %in% labs_excl),]

# # include only participants in cheat condition (design was 2x2, cheat - no cheat x commandment - books)
# pc_df <- pc_df[which(pc_df$maz.cheat.cond == "cheat"),]

```

The data-set contains a large number of information, that we do not need. Therefore, in substep *b)* we retain only those columns from the data-set, that contain information about the participants' responses to the HEXACO-items and in which laboratory the participant was assessed. Since some of the HEXACO-items are reverse-coded, we need to re-code half of the items, so the scale is identical across all items.

```{r, warning = F}
# b) Select relevant variables and re-code items that are coded in the opposite direction

# retain only those columns, which are needed for subsequent analysis.
pc_df <- pc_df[,c(which(names(pc_df) %in% c("source")),
                  grep("^hex", names(pc_df)))]

# recoding the hexaco items, that need recoding

for(i in grep("^hex", names(pc_df))){
  pc_df[,i] <- as.integer(pc_df[,i])
}
# these are the numbers of the items, that need recoding
items_hex_recode <- c(30, 12, 60, 42, 24, 28, 53, 35, 41, 59, 28, 52, 10, 46, 9, 15, 57, 21, 26, 32, 14, 20, 44, 56, 1, 31, 49, 19, 55, 48)
names_items_hex_recode <- paste0("hex", items_hex_recode) # pasting "hex" and number gives the column names

names_items_hex_recode_R <- paste0(names_items_hex_recode, "_R") # adding _R for names of items, that are recoded
pc_df[,names_items_hex_recode_R] <- 6 - pc_df[,names_items_hex_recode] # recode items that need recoding
```

Lastly, in substep *c)*, we remove all columns that contain responses on the other HEXACO-dimensions, which are not Conscientiousness. Therein, we have constructed a data-set, containing the responses on Conscientiousness from all participants that are part of the primary replication sample in RRR10.

```{r, warning = FALSE}

# c) select items belonging to Conscientiousness subscale

#Conscientiousness
items_hex_CO <- c(2, 26, 8, 32, 14, 38, 50, 20, 44, 56) # items in Conscientiousness subscale
names_items_hex_CO <- ifelse(items_hex_CO %in% items_hex_recode, paste0("hex", items_hex_CO, "_R"), paste0("hex",items_hex_CO)) # did item need recoding?
pc_hex_items_CO <- which(names(pc_df) %in% names_items_hex_CO) # select all items from Conscientiousness subscale, correctly coded

pc_df <- pc_df[,c(names_items_hex_CO, "source")]

# pc_df <- na.omit(pc_df)

labs <- unique(pc_df$source)

```

However, we find that some participants' responses on the Conscientiousness-items are missing in this data-set. While this was of no concern to the replication-authors, as the HEXACO-inventory served as a filler instrument, the same is not true for this tutorial. We will use Cronbach's Alpha to estimate score reliability. Cronbach's Alpha can not deal with missing responses. To keep it simple for this tutorial, we make use of list-wise deletion to remove any missing responses from the data-set. This means that the data for each participant, where any response on one or several items is missing is deleted from the data-set. Alternative approaches, that attempt to impute the missing responses while retaining the general covariance-structure of the data-set are available. Great examples include Multiple Imputation or Multivariate Imputation by Chained Equations. Singe imputation methods like Mean or Mode Imputation restrict the data-sets covariance structure, leading to estimation issues and biases in subsequent analyses.

```{r, warning = FALSE}
# d) dealing with missing responses

colSums(is.na(pc_df))

pc_df <- na.omit(pc_df)
```

Finishing step 0, we have constructed a data-set, containing only the responses of participants that belong to the replication authors' primary analysis sample, additionally removing missing responses from the data-set using listwise deletion. Subsequently, we can begin to follow the Boot-Err procedure.

### Step 1 - estimating score reliability

The first step in the Boot-Err procedure is to generate estimates of score reliability for each sample or administration site respectively. In the following code, we use an *sapply*-loop to compute Cronbach's Alpha separately for each laboratory in the data-set. Cronbach's Alpha is generally defined in the following @eq-alpha_formal (ref.).

$$\alpha=\frac{k^2\ \bar{\sigma}_{ij}}{\sigma^2_x}$$ {#eq-alpha_formal}

Here, $k$ corresponds to the number of items in the scale, $\bar{\sigma}_{ij}$ constitutes the average covariance between items and $\sigma^2_x$ constitutes the total score variance in the total test results.

However, computing the average item covariance can be computationally intensive, and a more efficient computation technique is available in @eq-alpha_fast (ref.).

$$\alpha = \frac{k}{k-1}\left(1-\frac{\sum_{i=1}^k\sigma^2_{i}}{\sum_{i=1}^k\sum_{j=1}^k\ \sigma_{ij}}\right) $$ {#eq-alpha_fast}

Here, $\sigma^2_{i}$ describes the variance of scores on item $i$, while $\sigma_{ij}$ describes the covariance between items $i$ and $j$. The equation in @eq-alpha_fast computes the same Cronbach's Alpha as in @eq-alpha_formal, but much more efficiently. By using *sapply* to quickly loop across all laboratories, we can efficiently generate a vector containing all estimates of Cronbach's Alpha for each administration site respectively.

```{r, warning = F}
alpha_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  C <- cov(dat)
  j <- dim(C)[1]
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  return(alpha)
  
})

alpha_vec
```

### Step 2 - estimating error score variance

After generating estimates of score reliability using Cronbach's Alpha, we can turn towards Step 2 of the Boot-Err procedure - estimating error score variance. According to CTT, error score variance is defined as described in @eq-error_var.

$$ {\sigma}^2_e = \left(1 - \rho_{xx'} \right) \sigma^2_x$$ {#eq-error_var}

This means, provided we generate an estimate of total score variance, we can use @eq-error_est to estimate error score variance, as we already have estimate of score reliability in the form of Cronbach's Alpha readily available.

$$ \hat{\sigma}^2_e = \left(1 - \hat{\alpha} \right) \hat{\sigma}^2_x$$ {#eq-error_est}

Here, $\hat{\alpha}$ describes the estimate of Cronbach's Alpha and $\hat{\sigma}^2_x$ constitutes an estimate of total score variance.

In the following code we, again, use an *sapply*-function to efficiently repeat the same computation for each laboratory. We quickly estimate the total score variance $\sigma^2_x$ by averaging the item scores separately for each participant (generating the test score) and computing the variance of these test scores. Since the *sapply*-function also returns a vector of $\sigma^2_x$-estimates, we can easily use @eq-error_est to generate a vector containing error score variance $\sigma^2_e$-estimates for each laboratory respectively.

```{r}

var_X_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  var_X <- dat %>% 
    rowMeans(.) %>% 
    var(.)
  
  return(var_X)
  
})

var_E_vec <-  (1-alpha_vec) * var_X_vec

var_E_vec

```

### Step 3 - construct Standard Errors using Bootstrapping

As described in the section *Analysis of error score variance - Boot-Err*, we use a random-effects meta-analysis to estimate and test for heterogeneity in error score variances. Since estimation of total score variance and its components is not variance-stable, we need to use a variance-stabilizing transformation. Bartlett and Kendall [-@bartlett1946] show that using the natural log, as in @eq-stable_var, is a suitable variance-stabilizing transformation for total score variance estimates. $$ T[\hat{\sigma}^2_x] = ln \left(\hat{\sigma}^2_x \right) $$ {#eq-stable_var}

While little evidence on variance-stabilizing transformations for score variance components is available, we have used the natural log for both the total and the error score variance component in our proposal for the Boot-Err [@beinhauer2025]. In the simulation scheme, we have not found a large bias in the estimates which would suggest that the transformation described in @eq-stable_var would be unsuitable for the variance component of error score variance $\sigma^2_e$. Accordingly, we are confident in our claim, that @eq-stable_varE describes a suitable variance-stabilizing transformation.

$$ T[\hat{\sigma}^2_e] = ln \left(\hat{\sigma}^2_e \right) $$ {#eq-stable_varE}

However, in order to run a random-effects meta-analysis, we not only need a variance-stable estimate, but also a Standard Error, quantifying our uncertainty in that estimate. While no analytic approximation of a Standard Error for log-transformed error score variance has been derived, we can use Bootstrapping techniques to generate robust Standard Error estimates for $T[\hat{\sigma}^2_e]$.

The package *boot* provides easy-to-use and efficient functions (including parallelization if desired) for Bootstrapping techniques [@boot]. In this tutorial, we make use of version 1.3-31. In order to use *boot*, we need to provide the package with a user-written function, which needs to do two things: i) allow the *boot*-package to sample from our data-set (we do so by `dat_boot <- data[indices,]`), and ii) compute the parameter on the re-sampled data-set that we want to boot-strap. In the following code-chunk, we show a function that does just that.

```{r}

ln_var_E_boot_func <- function(data, indices){
  
  dat_boot <- data[indices,]
  
  C <- cov(dat_boot)
  j <- dim(C)[1]
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  var_E <- var_X * (1 - alpha)
  
  ln_var_E <- log(var_E)
  
  return(ln_var_E)
  
}
```

Now that we have a function available for *boot* to re-sample our data and estimate $T[\hat{\sigma}^2_e]$ in the bootstrapped samples, we need to supply our user-defined function to *boot* and compute the Standard Error from the bootstrapped estimates. Essentially, what we do here is estimate the Standard Error of log-transformed error score variance, by computing the standard deviation of all bootstrapped estimates for a single laboratory, see @eq-bootSE.

$$ SE\left[T[\hat{\sigma}^2_e]\right] = \sqrt{\frac{1}{n}\sum_{i=1}^n(T[\hat{\sigma}^2_e]_i - \bar{T}[\hat{\sigma}^2_e])^2}$$ {#eq-bootSE}

Here, $n$ describes the number of bootstrap-samples drawn, whereas $\bar{T}[\hat{\sigma}^2_e]$ describes the mean of the log-transformed error score variance estimates of the $n$ bootstrap samples. In the following code-chunk, we use the *boot* package to estimate $SE\left[T[\hat{\sigma}^2_e]\right]$ from 3000 bootstrap samples for each laboratory respectively. Again, we make use of an *sapply*-function to loop efficiently across laboratories.

```{r, warning = FALSE}


ln_var_E_SE_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_E_boot_func,
                               R = 3000)
  
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})
```

Lastly, we still need to formally transform and store the individual estimates of $T[\hat{\sigma}^2_e]$ for each laboratory (so far we only did this for the bootstrap-samples). However, at this point, computing this transformation is trivial. To make things easier for us in the following steps, we combine the estimates of log-transformed error score variance and its Standard Errors here in a single data-frame. We add the laboratory names to the data-frame, so estimates can be linked to their respective sources.

```{r, warning = FALSE}
ln_var_E_vec <- log(var_E_vec)

ln_var_E_df <- data.frame(ln_var_E = ln_var_E_vec, 
                          ln_var_E_SE = ln_var_E_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

ln_var_E_df
```

Concluding step 3, we have gathered all estimates we need, to perform a random-effects meta-analysis on (log-transformed) error score variances.

### Step 4 - meta-analysis of error score variance

For readers who only wish to use Boot-Err, Step 4 is very simple \[Ich habe mal drüber nachgedacht hier als zusätzlichen input das random-effects model zu erklären - der Part hier ist aber doch etwas länger geworden, braucht's das in dem Sinne denn überhaupt?\]. We use the R-package *metafor*, as it provides an efficient and easy-to-use implementation of random-effects meta-analyses for any parameter [@metafor]. Using the *rma*-function, with the argument *method = "REML"*, ensures that we use the REML-estimator of heterogeneity. Hönekopp and Linden [-@hoenekopp2022] have shown that the REML-estimator tends to outperform other estimators across a wide range of conditions. Therefore, the following chunk will run a random-effects meta-analysis in *metafor*, generating an estimate of the meta-analytic mean (in the output `estimate`), estimates of absolute heterogeneity in standard deviation and variance (in the output `tau` and `tau^2`) and estimates of relative heterogeneity in the form of $I^2$ and $H^2$ (in the output `I^2` and `H^2`). Additionally, *metafor* provides the results for two hypothesis tests. Firstly, the test for heterogeneity (`Test for Heterogeneity` in the output), assessing whether the estimate of heteorgeneity $\tau^2$ can be statistically distinguished from $0$. Secondly, the test for the meta-analytic effect size, assessing whether the meta-analytic aggregate can be statistically distinguished from $0$ (in the Output `Model results`).

```{r}

ln_var_E_rma <- metafor::rma(data = ln_var_E_df,
                             yi = ln_var_E,
                             sei = ln_var_E_SE,
                             method = "REML")

ln_var_E_rma

```

The significance tests indicate two things for the log-transformed error score variance of Conscientiousness across replications in Verschuere et al. [-@RRRMazar]: Firstly, we find that the meta-analytic estimate of `r paste0("$\\mu_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$b[1], 3), "$")` is significantly different from zero (`r paste0("$z = ", round(ln_var_E_rma$zval, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$pval < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$pval, 3))), "$")`), using a significance level of 5%. However, this is not meaningful information, as the estimate is on the log-transformed scale and can not be interpreted. Secondly and more interestingly, we find that the test for heterogeneity indicates that there is statistically significant heterogeneity in log-transformed error score variances (`r paste0("$Q(", ln_var_E_rma$k - 1, ") = ", round(ln_var_E_rma$QE, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$QEp < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$QEp, 3))), "$")`). Even though the tested estimates are still on the log-scale, we can nonetheless infer from this result, that error score variance $\sigma^2_e$ is not identical across all administrations of the Conscientiousness-scale. However, the estimate of `r paste0("$\\tau^2_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$tau2, 3), "$")` is uninterpretable due to the log-transformation.

The estimates of relative heterogeneity can be interpreted despite the transformation. The estimate of `r paste0("$I^2 = ", round(ln_var_E_rma$I2, 2), "$")` implies that `r round(ln_var_E_rma$I2, 2)`% of the variance observed in the estimate of error score variance are due to actual heterogeneity in the population, not due to sampling error. Similarly, the estimate of `r paste0("$H^2 = ", round(ln_var_E_rma$H2, 2), "$")` implies that the heterogeneity is `r round(ln_var_E_rma$H2, 2)` times as large as the variance induced by sampling error. According to standard conventions (ref.), both estimate imply a strong or large extent of heterogeneity. However, both estimates of relative heterogeneity are often criticized, as they depend strongly on sample size and are easily manipulated (refs.). Instead, we should turn towards an informative interpretation of the absolute heterogeneity in terms of $\tau^2$ or $\tau$.

To do so, we need to back-transform our newly generated estimates to the original scale. This is done in Step 5.

### Step 5 - Backtransforming estimates

The transformation of score variance (components) using the natural log not only serves to generate variance-stable estimates. Additionally, it comes with the added benefit, that we can assume $T[\sigma^2_e]$ to follow a normal distribution. Several authors have proposed that the total score variance, if not constant, can be described by a log-normal distribution (refs.). This means that the log-transformed estimates accordingly follow a normal distribution. This is very neat for the application of the Boot-Err technique, as one central assumption of a random-effects meta-analysis is that the parameter follows a normal distribution. Additionally, it helps us back-transform the estimates of meta-analytic mean $\mu_{T[\sigma^2_e]}$ and its heterogeneity $\tau^2_{T[\sigma^2_e]}$, as equations are readily available (refs.).

$$ \hat{\mu}_{\sigma^2_e} = \exp\left(\hat{\mu}_{T[\sigma^2_e]} + \frac{1}{2} \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_mu}

@eq-backtransf_mu demonstrates how the mean error score variance $\hat{\mu}_{\sigma^2_e}$ can be estimated from the meta-analytic estimates of $T[\sigma^2_e]$. Here, $\hat{\mu}_{T[\sigma^2_e]}$ represents the estimate of meta-analytic mean of log-transformed error score variance and $\hat{\tau}^2_{T[\sigma^2_e]}$ describes the estimate of its heterogeneity. Similarly, @eq-backtransf_var can be used to derive the heterogeneity of error score variance $\hat{\tau}^2_{\sigma^2_e}$ on its original scale, from the same meta-analytic estimates of \$T\[\sigma\^2_e\]\$\$.

$$ \hat{\tau}^2_{\sigma^2_e} = \exp\left(\hat{\tau}^2_{T[\sigma^2_e]} - 1\right) \cdot \exp\left(2 \hat{\mu}_{T[\sigma^2_e]} + \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_var}

In the following chunk, we define two functions which compute [Equations -@eq-backtransf_mu] and [-@eq-backtransf_var] from the *metafor*-output. The functions only take the *metafor*-object generated from the *rma*-function as input and return the estimates $\hat{\mu}_{\sigma^2_e}$ and $\hat{\tau}^2_{\sigma^2_e}$ respectively.

```{r}
backtransform_var_mu <- function(rma_obj){
  exp(rma_obj$b[1] + (.5*rma_obj$tau2))
}

backtransform_var_tau2 <- function(rma_obj){
  (exp(rma_obj$tau2) - 1) * exp((2 * rma_obj$b[1]) + rma_obj$tau2)
}

backtransform_var_mu(ln_var_E_rma)

backtransform_var_tau2(ln_var_E_rma) %>% 
  sqrt(.)
```

Running the two functions in the chunk above returns two estimates: The meta-analytic mean error score variance of the HEXACO-Conscientiousness scale across 19 laboratories is $\hat{\mu}_{\sigma^2_e} = `{r} round(backtransform_var_mu(ln_var_E_rma), 3)`$; The estimate for its heterogeneity is $\hat{\tau}_{\sigma^2_e} =`{r} round((backtransform_var_tau2(ln_var_E_rma) %>%   sqrt(.)), 3)`$. To understand what this implies, we can assess the relative size of heterogeneity, also known as the coefficient of variation, which is computed as $CV = \frac{\tau}{\mu}$. Here we find a CV of $CV_{\sigma^2_e} = `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3)`$. This means that the extent of the differences in error score variances is approximately `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance found across these laboratories.

None of the other approaches, such as RG-MA, IRT or measurement invariance testing allow to generate these estimates we have derived using the Boot-Err technique. For researchers who are more accustomed to these scales, these estimates can be highly valuable to understand how much variance in scores we can expect to be introduced by measurement imprecision across administrations of the same scale. Since we are no experts in use of the Conscientiousness-scale, we have to look slightly further beyond. However, as described in the following Bonus-Step, we can follow the Boot-Err technique with an additional parameter, generating much more informative insights.

### Bonus-Step - Relating error score variance to total score variance

Score reliability $\rho_{xx'}$ is defined as the relative extent to which the score variance observed is attributable to actual differences in the true scores, not to measurement imprecision. However, that also means that $1 - \rho_{xx'}$ describes the relative extent to which the score variance is attributable to random measurement error.

Similarly, on the meta-analytic scale, we can look at the heterogeneity in total score variance $\sigma^2_x$, and attempt to understand how much of its heterogeneity came to be through differences in error score variance. Intuitively, many researchers tend to ascribe heterogeneity in total score variance $\sigma^2_x$ to differences in measurement precision. This means that often, a larger score variance is interpreted as if it came to be through larger measurement error such as $\sigma^2_e$. However, obviously parts of the heterogeneity might also be better explained by differences in the true score variance $\sigma^2_t$. Fortunately, if we follow Steps 3 through 5 for the total score variance $\sigma^2_x$ instead, we can assess in how far its heterogeneity can be attributed to differences in true score variance $\sigma^2_t$ or measurement error $\sigma^2_e$.

In the following chunk, we briefly run through these steps to estimate heterogeneity in total score variance $\sigma^2_x$.

```{r}
ln_var_X_boot_func <- function(data, indices){
  
  dat_boot <- data[indices,]
  
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  ln_var_X <- log(var_X)
  
  return(ln_var_X)
}


ln_var_X_SE_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_X_boot_func,
                               R = 3000)
  
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})

ln_var_X_vec <- log(var_X_vec)

ln_var_X_df <- data.frame(ln_var_X = ln_var_X_vec, 
                          ln_var_X_SE = ln_var_X_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

ln_var_X_rma <- metafor::rma(data = ln_var_X_df,
                             yi = ln_var_X,
                             sei = ln_var_X_SE,
                             method = "REML")


backtransform_var_mu(ln_var_X_rma)

backtransform_var_tau2(ln_var_X_rma) %>% 
  sqrt(.)

```

We find a meta-analytic mean of total score variance $\hat{\mu}_{\sigma^2_x} = `{r} round(backtransform_var_mu(ln_var_X_rma), 3)`$ and a heterogeneity of $\hat{\tau}_{\sigma^2_x} = `{r} round((backtransform_var_tau2(ln_var_X_rma) %>% sqrt(.)), 3)`$. Under CTT, total score variance is defined as the sum of true and error score variance $\sigma^2_x = \sigma^2_e + \sigma^2_t$, which are assumed to be independent. This means that we can assess how much heterogeneity in total score variance can actually be explained by differences in measurement error:

$$ R = \frac{\hat{\tau}^2_{\sigma^2_e}}{\hat{\tau}^2_{\sigma^2_x}} $$ {#eq-ratio}

In @eq-ratio, we define the ratio-variable $R$ as the ratio of $\hat{\tau}_{\sigma^2_e}$ to $\hat{\tau}_{\sigma^2_e}$. For Conscientiousness, we find a ratio of $R  = `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3)`$. This means that about `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3) * 100`% of heterogeneity in $\sigma^2_x$ could be explained by differences in random measurement error. This also means that `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`% of this heterogeneity needs to be explained by differences in true score variance $\sigma^2_t$.

### Conclusion

Using the Boot-Err technique, we can use random-effects meta-analysis and appropriate variance-stabilizing transformations paired with Bootstrapping techniques to assess in how far there is heterogeneity in random measurement error across administrations of the same scale. Making use of the data supplied by Verschuere et al. [-@RRRMazar], we used Boot-Err to understand in how far there are differences in absolute measurement precision across 19 laboratories where the HEXACO-Conscientiousness personality scale was administered. We not only find statistically significant differences in error score variance, but were also able to estimate the absolute and relative extent of these differences. Therein, we are certain that the random measurement error in the Conscientiousness scores is not constant across samples. We find that the heterogeneity is about `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance. Across administrations of the same scale, error score variance easily varies between `r round(exp(ln_var_E_rma$b[1] - sqrt(ln_var_E_rma$tau2)), 3)` and `r round(exp(ln_var_E_rma$b[1] + sqrt(ln_var_E_rma$tau2)), 3)` (mean value plus/minus heterogeneity, then back-transformed) \[add this to Bonus-Step?\].

In order to better understand what these differences mean, we additionally uncovered in how far the heterogeneity in total score variance can be attributed to these differences in measurement error. Here, we found that the overwhelming majority of heterogeneity, `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`%, can not be explained by differences in measurement error. For the Conscientiousness-scale, this implies that, while there clearly are substantial differences in measurement error across groups, the distribution of true scores varies much more strongly across groups. This is important, as this implies that the differences in total score variance can not be attributed to differences in measurement error. Instead, Conscientiousness is distributed highly different across those groups. In Beinhauer et al. [-@beinhauer2025liable] we discuss potential causes and implications of this phenomenon.

## Discussion

Measurement precision and how strongly it diverges across administrations of the same instrument is a central question in social and behavioural sciences. Differences in random measurement error are not only interesting to researchers assessing the quality of a measuring instrument. It also has clear implications when it comes to hypothesis testing and standardization of effect sizes. Therein, the assessment of measurement equivalence has implications for replication studies and meta-analytic applications. Across the first pages, we have summarised the most popular techniques to assess differences in random measurement error - Reliability Generalization Meta-Analysis, Measurement Invariance Testing and approaches stemming from Item Response Theory. Additionally, we introduced Boot-Err, a technique we developed to step away from relative measures such as score reliability or tests without estimates of how large the differences are, like Measurement Invariance Tests [@beinhauer2025].

In this tutorial, we have demonstrated how the Boot-Err can be used to generate detailed estimates and tests regarding the extent of the differences we observe in random measurement error, and how strongly they affect the differences in score variance. It is important to note that this technique comes with a number of limitations and constraints to generalizability. Just like for any statistical analysis, a number of assumptions need to be met. Regarding application of the Boot-Err, total and error score variance should follow a log-normal distribution and be computed from samples that are generally independent of each other (the individual samples should not be described as if stemming from a nested structure). Additionally, a number of assumptions relate to the type of score reliability estimate used. Here, we made use of Cronbach's Alpha, which typically comes with assumptions of tau-equivalence and unidimensionality (ref.). While these assumptions are widely criticized and rarely hold up in real data (ref.), we need to remind ourselves that these assumptions also relate to typical scoring methods of these measuring instrument. The HEXACO-personality score for each dimension is computed by taking the average of the relevant item scores \[check to make sure this is correct\]. Therein, even stricter assumptions of tau-equivalence and unidimensionality apply - making Cronbach's Alpha the more appropriate choice of score reliability estimate over other ones that may impose less strict assumptions.

Even though differences in absolute measurement precision are of central interest to measuring instrument and scale development, methods to assess differences in error score variance each come with their own caveats and shortcomings. Models regarding differences in score reliability fail to take differences introduced by true score variance into account, while models simply testing for differences in error score variance without actually estimating it fail to put these differences into perspective. Perhaps this is the reason why discussions surrounding the impact of random measurement error on standardized effect sizes and hypothesis tests has been overemphasized (refs. \[NEED TO PICK THIS UP IN INTRO\]), implying that removing random measurement error was akin to harmonizing variability across populations and data-sets.

Boot-Err, mends both these shortcomings, as differences in error score variance can be estimated and tested, but also put into perspective on how much heterogeneity in total score variance they actually produce. As we demonstrated in the Bonus-Step section, differences in error score variance can not explain why total score variance differs across groups--instead, differences in true score variance must come at a much larger extent. This implies that differences in score variability can not sufficiently be mended by taking care of measurement error. Instead, future research should ensure that the true score variance is stable across groups and representative for the respective population.

#### Abstellgleis

DAS HIER KÖNNT IHR IGNORIEREN - DAS SIND ALTE TEXTBAUSTEINE DIE FÜR DEN MOMENT KEINEN PLATZ HABEN

One central assumption of IRT is that the item characteristics are valid for all subgroups in the population to which the model is applied. This means that, no matter the sub-group the individual is from, the item-response-function should be identical. This means that all individuals with the same latent ability score $\theta$ have the same probability of getting an item right or wrong. As this is a central assumption of these models, routine procedures for examining differential item functioning (DIF) have been established. Typically, a *reference sample* should be available, where an IRT-scoring model has been established. Subsequently, we can assess if any items exhibit DIF in a *focal group*, where the same $\theta$ would lead to different observed scores.

## References

::: {#refs}
:::
