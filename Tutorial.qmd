---
title: "Tutorial"
format: latex
bibliography: bibliography/Boot-Err_Tutorial.bib
csl: bibliography/apa-6th-edition.csl
---

# New

Intro - initial question / that is best answered using Boot-Err

## Introduction

Researchers frequently seek to assess the consistency and accuracy of psychological measurements across different administrations of the same scale.

For instance, a psychological test measuring Extraversion might be administered to a group of participants in a German sample of Psychology students and a random sample from an international online panel for market research. Understanding whether the instrument produces results of a similar quality across different administration contexts is central to evaluating the generalizability of the instrument.

### Reliability generalization meta-analysis

A simple and accessible way to compare measuring quality across administrations of the same scale is by comparing the reliability coefficients computed for each administration sample.

A formal method to assess whether reliability coefficients of the same instrument differ across administrations has been developed in the form of reliability generalization meta-analysis.

Reliability generalization meta-analysis allows researchers to test, whether the differences observed in reliability coefficients go beyond those introduced merely by sampling error.

Reliability coefficients estimate score reliability - the ratio of true to error score variance. \[... therefore, from reliability generalization meta-analysis, we learn that these ratios differ across administrations of the same instrument. What we cannot learn from this analysis is in how far these differences in random measurement quality are due to differences in the underlying structure of what we try to assess or how well the items measure said structure. Similarly, we cannot infer is in how far the size of the random measurement error differs across administrations.

### Measurement invariance testing

Reliability generalization meta-analysis is limited to modelling and inspecting the differences in standardised reliability coefficients. Measurement invariance testing attempts to go beyond that, modelling and assessing in how far the measuring instrument captures the same underlying latent trait.

In a series of four (sometimes more) steps, different levels of measurement invariance are tested.

Thereby, measurement invariance testing allows for a more nuanced discussion on whether the measuring instrument produces scores of equal quality across administrations.

However, measurement invariance testing does not allow for a quantification of how large the differences in measurement error are across the different administration sites

### Boot-Err - inspecting differences in random measurement error

We developed Boot-Err to go beyond ratios of variance components. Instead, we specifically aimed to assess firstly whether we can identify statistically significant differences in error score variances. Additionally, we were interested in a quantification of the extent of these differences, by means of heterogeneity.

Using Boot-Err, we can identify whether the random measurement error introduced by our measuring instrument differs across the different administrations of the measuring instrument.

This analysis of random measurement error is independent of how homo- or heterogeneous;y the different samples are composed.

The analysis using Boot-Err cannot inform us on whether our measuring instrument captures different latent factors or which specific item loads differently onto which factor. The test for heterogeneity in error score variances could essentially be described as a quantification of the extent of measurement non-invariance identified in the fourth step in the tests for measurement invariance.

Imagine we want to conduct a study on how the Big 5 personality dimensions are distributed across a number of countries. To do so, we compare several studies where the Big 5 personality inventory was administered in samples in different countries.

However, before delving into the analysis, we want to make sure that the personality traits were measured adequately precise in each study. Therefore, we want to focus on random measurement error. We assume that the measures are generally unbiased. We are aware that the scores derived from a measuring instrument are not a perfect representation of an individual's personality. However, we believe that there is no systematic error leading to scores that tend to be too high or too low in some of the samples.

Different approaches to this question -\> different aspects of measuring quality

individual approaches only brief discussion

# Old

## Introduction

Measurements are central to the empirical sciences. Across these scientific fields, precision is a crucial concept to describe an aspect of the quality of measurements derived from an instrument. In the social sciences, measurements typically take the form of scores derived from some kind of inventory, consisting of a number of items assessing at least one underlying construct, where participants either respond themselves or are evaluated by some third party. It is well known that, even if a single “true score” on a single construct exists for each measured individual, the observed score does not truly represent this “true score”. Instead, even though on average we might get it right, the observed score will always be off by some extent. Precision is a crucial concept to describe an aspect of the quality of measurements derived from an instrument: the degree to what extent the observed score is off from the “true score”.

Most often, this measurement precision is evaluated using score reliability. While different definitions and sub-categories of score reliability exist in parallel, all parameters of score reliability attempt to describe the same ratio: a ratio of two variance parameters, one that attempts to describe the total variance in the observed scores and one that attempts to describe the variance in the true scores or the latent trait that we try to measure. In the context of Classical Test Theory (CTT), where an observed score $X$ is made up of the sum of true score $T$ and error score $E$, score reliability takes the form of $\rho_{xx’} = \frac{\sigma^2_T}{\sigma^2_X}$. In the context of structural equation or factor models (SEM), score reliability typically takes the form of $\rho_{xx’} = \frac{\sigma^2_\eta}{\sigma^2_X}$, where $\eta$ describes the underlying latent factor. Item response theory (IRT) aims to model a similar latent ability factor $\theta$. While in IRT precision is less often described in terms of score reliability, it can be described as $\rho_{xx’} = \frac{\sigma^2_\theta}{\sigma^2_X}$, taking essentially the same form as the definitions under CTT or SEM.

What all these forms of reliability have in common is that they try to quantify all the variance that is distinct from random noise, relative to the total variance observed in the manifest scores. Therein, they describe the relative amount of variance that is due to differences in $T$, $\eta$, or $\theta$. This leads to a parameter space between 0 and 1 for the score reliability, where a 0 indicates that all the observed score variance is in fact random noise, while the 1 indicates that there is no random noise in the observed scores. It is important to note that, no matter the score framework used, reliability is not an aspect of a measuring instrument, but an aspect of the scores created by administering a specific measuring instrument to a specific population. Score reliability is the most popular parameter to evaluate that quality of a measuring instrument. Including and beyond score reliability, several approaches have been proposed that allow for a comparison of precision in scores produced by a measuring instrument across different groups or populations.

Over the following paragraphs, we first introduce and discuss the most popular techniques to model and test for these differences in measurement precision. In doing so, we focus on the well-established models of Reliability Generalization, Measurement Invariance Testing and Techniques involving Item Response Theory. Additionally, we introduce Boot-Err, a technique we developed that closes some of the gaps left behind by these more popular approaches. After summarising the individual advantages and disadvantages of the different techniques, we provide a tutorial in the statistical programming language R. Therein, we aim to make users more familiar with the Boot-Err approach and provide the general code to employ the technique oneself. We close with a discussion on the potential and shortcomings in the context of measurement precision analysis.

## Approaches to identify differences in measurement quality

### Reliability Generalization Meta-Analysis

As score reliability is the most widely reported assessment of measurement precision in the literature, an approach has been developed to model score reliability estimates. Referred to Reliability Generalization Meta-Analysis entails the collection of score reliability estimates derived from the administration of the same instrument across several different samples. Using an appropriate variance stabilising transformation (Bonett), a random-effects meta-analysis can be used to generate an estimate of mean score reliability corrected for sampling error. Additionally, an estimate of heterogeneity, the population variance of the score reliability across samples, also corrected for sampling error, can be extracted. The meta-analytic model is typically paired with Cochran’s Q-test, a significance test to assess whether the estimate of heterogeneity can be statistically differentiated from zero.

Therefore, Reliability Generalization Meta-Analysis allows for an assessment to what extent the differences in score reliability observed across several samples do actually constitute “true” differences in score reliability. This procedure is highly accessible, as aggregate information in the form of estimates of score reliability and its standard error are sufficient for analysis. No individual participant data is required. However, this also means that Reliability Generalization Meta-Analysis can only inform us on differences in relative precision, as no information beyond score reliability is available. More diverse samples produce scores of larger score reliability, even if measurement precision is identical. Assessing score reliability obfuscates this. Even if measurement precision is identical across different groups/populations, if the groups/populations vary in how diverse they are in terms of true scores or the distribution of the latent trait, Reliability Generalization Meta-Analysis will suggest that measurement precision is heterogeneous,.

### Measurement Invariance Analysis

While Reliability Generalization Meta-Analysis can identify differences in score reliability across administrations, it can not identify differences in absolute error score variance. Alternatively, factor analysis approaches like structural equation modelling (SEM) or confirmatory factor analysis (CFA) provide tests specifically for such differences. The approach known as measurement invariance was initially proposed by Byrne, Shavelson and Muthen (1989). Using confirmatory group factor analysis (which is often implemented in a SEM-framework) the fit of a model is tested repeatedly. Starting with a completely free model, with each iteration more parameters are fixed. Using a null hypothesis significance test (NHST) or some other metric for which cut-off values are available, we can assess whether the model fits the new constraints significantly worse. In that case, measurement invariance for that particular step is not given.

Typically, a sequence of four tests, assessing four facets of measurement invariance, are completed. Often, initially a CFA-model describing how the latent factor is operationalized by the individual items is modelled, where factor loadings, item intercepts and residual variances are free to vary across groups. This model is evaluated against a completely free model, where the factor structure is not predetermined. If the more restrictive CFA-model fits reasonably well, it suggests that the factor structure holds across groups, this step is often referred to as configural invariance. If the model does fit worse, typically no further, more restrictive tests of invariance are undertaken, as the items are noninvariant at the lowest level. Subsequently, a second, slightly more restrictive model is defined, in which the factor loadings are constrained to be identical across groups. This model is evaluated against the previous model, which determined the configural invariance. If the more restrictive model fits sufficiently well, this indicates that the relationship between observed item score and latent factor is the same across groups, often referred to as metric invariance. The next model, where additionally the item intercepts are constrained to be identical across groups is again tested against the previous model determining metric invariance. If this model fits, it demonstrates that the differences in observed scores can not be explained by differences in how individual respond to the items across groups (e.g. avoiding high or low scores in one or a few, but not all groups), and is referred to as scalar invariance. Lastly, the residual variance are additionally constrained to be equal across groups and such a model is, again, tested against the previous model establishing scalar invariance. If this model fits sufficiently well, it indicates that the random noise observed in the scores, that could not be explained by the latent factor and how it relates to the observed score, is identical across groups. If this holds, it is often referred to as residual invariance.

This last level of measurement invariance, the test for residual invariance, comes close to what a test for heterogeneity in a Reliability Generalization Meta-Analysis does. The preceding levels dealt with the underlying factor structure, the systematic measurement issues (metric invariance) and differences in the group mean response (scalar invariance). Residual variance deals determines whether the amount of random noise in the observed scores is (more or less) identical across groups/populations. If residual variance can not be achieved, this indicates that, in terms of random measurement error, the instrument produces scores of varying quality across the different groups/populations. Unlike a test for heterogeneity in score reliability, this is not necessarily affected by differences in the distribution of the underlying true scores (true score variance or latent factor variance). However, it is important to note that, whether this test is noninvariant to these differences depends on how the model was identified initially.

As measurement invariance tests are typically performed using the SEM-framework, this implies that some parameter needs to be fixed to a specific value, for the model to be identified. Essentially, as the latent factor scores are unobservable, they can take any arbitrary value – instead, we have to assign meaning to the factor scores by scaling them in some way. Typically, this is done by choosing one of two procedures:

\(1\) the model is identified by fixing one of the factor loadings to 1 (or any value for that matter). This means that the factor scores are defined on the basis of the scores in that specific item. However, this also means that this item is essentially responsible for ensuring that the whole model is specified correctly. If, for example, the factor loading for the first items was fixed, and this item happens to be defunct, as it holds little relationship to the underlying construct or as it is interpreted differently across the different groups/populations, the specified model will not fit to the data very well. In that case, all invariance tests, starting with the first regarding configural invariance, are biased and bound to lead to uninterpretable results.

\(2\) Alternatively, we can scale the latent factor scores by setting the latent factor variance to 1 (or any value for that matter). As this needs to be done for every group individually, this implies that the variance of the latent factor scores is assumed to be identical across those groups, and the model is estimated accordingly. If, as discussed for the RG-MA, the true score variance is actually not identical across groups, the same is true for the latent factor scores. In that case, the estimates of factor loadings and residual variance will be biased. Similar to RG-MA, this would lead to conclusions that the instrument’s scores diverge in measuring quality (both in random and systematic measurement quality) across the groups, even though this is not necessarily the case.

Therein, if a valid identification strategy is available, measurement invariance techniques allow for valid inference concerning the presence of differences in random measurement error across groups. A combination of significance tests and metrics with established cut-off criteria, which help identify whether error variance heterogeneity is present or not. However, no metric is available that assess how large those error variance differences are and how strongly they affect the observed scores. Additionally, some of the assumptions underlying the RG-MA approach underly the measurement invariance approach as well, depending on how the model is to be identified (either (essential) tau-equivalence or homogeneous true score variance/latent factor variance).

### Item Response Theory

Item response theory, introduced by Alfred Binet (ref. 1900s) and Louis Thurstone (ref. 1920s), refined by Lord (ref. 1950s) and Rasch (ref. 1960s), puts ideas similar to latent factor models in SEM central. The general idea is that an unobserved characteristic, such as an underlying latent trait or ability score $\theta$, causes differences in the observed scores generated from the test's items. Additionally, each item is assumed to have different characteristics, such as item difficulty or how well the item discriminates between different levels of underlying ability $\theta$. Typically, an s-shaped curve known as item-response-function is used to describe the relationship between $\theta$ and the observed score $x$. Depending on the item characteristics, the function can take different shape--e.g. the slope may be less steep for an item which discriminates less strongly, or the slope may be moved further to the right for more difficult items. Most popular models used in IRT are logistic models, where the probability of responding correctly to an item is modelled, across a range of ability scores $\theta$. The simpler 1-pl model (also known as "Rasch-model") only takes item difficulty into account, while more complicated models such as the 3-pl model additionally take item discrimination and random guessing into account.

Highly sophisticated techniques to identify systematic measurement error, known as differential item functioning, have been developed (refs.). However, approaches to systematic measurement error are beyond the scope of this paper, as we focus on measurement precision or random measurement error. The focus of IRT-modelling typically lies on estimating latent ability scores $\theta$. As random measurement error is easily minimized by using a larger number of items, taking longer tests, little research has been done on detecting differences in random measurement error across subgroups taking the same tests. Generally, estimates of reliability for different aspects of the IRT-model are available. Therein, estimates of latent score reliability, item difficulty reliability or item discrimination reliability can be generated. This allows us to assess how much random error in the observed scores can be attributed to e.g. item difficulty. However, we are not aware of any models that assess differences in these estimates of score reliability across sub-groups taking the same test within the IRT-framework. However, as long as a sufficiently large number of subgroups is available, Reliability Generalization Meta-Analysis should be applicable to the specific type of reliability.

### Analysis of error score variance - Boot-Err

Alternatively, we have developed an alternative approach to discuss the question of differences in measurement quality across administrations of the same instrument. Rooted in CTT and inspired by shortcomings of the RG-MA literature and identification issues in measurement invariance testing, we proposed a novel method to assess error score variance directly [@beinhauer2025]. Assuming that full individual participant data is available, which is required for the measurement invariance and IRT approaches as well, Boot-Err estimates and tests for heterogeneity error score variance. As this manuscript serves as a tutorial to help researchers understand how Boot-Err is used in practice, here a rather short description of how Boot-Err generally works follows.

By collecting estimates of score reliability, e.g. Cronbach’s Alpha, the Greatest Lower Bound or an Intraclass Correlation, these can be used to generate estimates of error score variance for each group/sample respectively. As for score reliability, estimates of variances and variance components are not variance-stable. This means that the size of their individual sampling error (SE) depends on the size of the variance component itself – a larger error score variance therefore automatically comes with a larger standard error. Boot-Err employs traditional random-effects meta-analytic models to discuss heterogeneity. As such models make use of the standard error to weigh individual estimates and estimate heterogeneity, without variance-stable parameters, estimates and tests are bound to be biased. Therefore, a variance-stabilizing transformation of error score variance estimates is performed, simply by taking the log of the individual estimate. Using bootstrapping, we can derive robust standard errors for these transformed estimates of error score variance.

Using a random-effects meta-analysis, we can estimate the mean (transformed) error score variance and its heterogeneity. Additionally, we have derived equations, which can be used to back-transform these estimates to the initial scale of error score variances. Therein, to our knowledge, this is the only procedure that actually generates an estimate to what extent such differences are found in error score variance. Using Cochran’s Q-test for heterogeneity, we can assess in how far the estimate of heterogeneity can be distinguished from zero using a criterion of statistical significance. Similar to the approaches of measurement invariance and IRT, Boot-Err allows for an NHST, targeting whether differences in measurement quality in terms of random error can be found. Lastly, by following the same steps for the total score variance (performing a random-effects meta-analysis on transformed estimates and back-transform mean and heterogeneity), we can compute how much heterogeneity in the total score variance could actually be attributed to differences in random measurement error. Therein, Boot-Err also allows for a relative assessment, in how far differences in random error are responsible for the general heterogeneity observed in score variances or effect sizes.

### What inference does each method allow?

| Approach | Distinctive Assumption | Appropriate Inference |
|------------------------|------------------------|------------------------|
| Reliability Generalization Meta-Analysis | \- Essentially parallel items <br> - Appropriately modelled distribution of reliability coefficients | \- Presence of differences in score reliability across administrations <br> - Estimate of extent of those differences |
| Measurement Invariance (Residual Invariance step) | \- Appropriately identified model (correctly identified stable item variance OR stable latent score variance across administrations) | \- Presence of differences in random error score variance across administrations |
| IRT | \- Homogeneous subpopulations <br> | \- Presence of differences in specific score reliability component across administrations |
| Boot-Err | \- Essentially parallel items <br> - Appropriately modelled distribution of variance components | \- Presence of differences in random error score variance across administrations <br> - Estimate of extent of those differences |

Across the previous paragraphs, we have highlighted how RG-MA, Measurement Invariance Testing, IRT Reliability Approaches and Boot-Err take different approaches to discuss (more or less) the same question: Does measurement quality diverge across several administrations of the same instrument. However, we have also outlined that the different approaches do not allow for the same kind of inference and estimate. This implies that they are not all suited equally well to assess these differences in measurement quality, even though we already restricted ourself to questions of random measurement error. Table 1 summarises the differences across the four approaches in terms of distinctive assumptions underlying the individual approach, and the appropriate conclusion, that can be drawn from each approach individually.

If we are only interested in discussing relative random measurement error across administrations, then RG-MA is the most appropriate approach. Assuming that score reliability coefficients are modelled appropriately (which necessarily requires a transformation of coefficients) and items are essentially parallel (which is an assumption underlying the transformation of reliability coefficients, not the estimation of the coefficients themselves), RG-MA provides a robust NHST, detecting differences in score reliability across administrations. Additionally, it provides and estimate of heterogeneity of score reliability coefficients, which can inform us on how much divergence we can expect across administrations of the same scale to similar populations.

If we are interested in absolute random measurement error, on the other hand, RG-MA is not a valid approach. Instead, users can turn towards the established methods of measurement invariance testing. The test for residual invariance (also known as full uniqueness invariance) assesses in how far the residual item variance differs across administrations of the same instrument. Assuming that the model is appropriately identified (keeping the implications of restricting either factor loading or latent factor variance in mind), residual item variance describes essentially the same as random error score variance. Therein, measurement invariance testing allows for an NHST, but also tests using other indicators with established cut-offs, both of which allow to identify whether differences in random measurement error persist across administrations.

However, both IRT and measurement invariance testing do not allow for an assessment of the extent of those differences in random measurement error. If we are interested in estimating how strongly the random error score variance varies across administrations, we can turn towards Boot-Err. Assuming that the items are essentially parallel (which is more restrictive than assumptions underlying the measurement invariance approach) and that the variance components are appropriately modelled, Boot-Err supplied both an estimate of the extent of differences in error score variance across administrations and an NHST for that heterogeneity. Generally, as Boot-Err was developed specifically to fill that gap, to our knowledge it is the only approach that allows for an estimate of heterogeneity in error score variance.

However, it is important to keep in mind that the distinctions drawn between these approaches are overemphasised, to highlight which inference can be drawn from each individually. At the same time, the approaches do not contradict each other. In our proposal for Boot-Err, we discuss how a typical RG-MA is best enriched by performing a Boot-Err additionally, highlighting how those differences in score reliability might have come into play. Similarly, if estimates of residual variance are derived from the models constructed in the measurement invariance testing procedure, a modified approach of Boot-Err might be used to estimate the extent of heterogeneity in residual variance across administrations. (The same is true for IRT?)

## Tutorial

### Step 0 - Preparing Data

Since each data-set is different, depending on how data was collected and possibly aggregated, Step 0 is not formalized in the Boot-Err approach. Instead, the data needs to be cleaned and appropriately manipulated beforehand. By default, the Boot-Err approach makes use of list-wise deletion, if any observation for an item is missing. This tutorial makes use of data in a wide format. Concerning list-wise deletion, this implies that if a participant fails to respond to one or several items from the measuring instrument, all data from that participant is disregarded. Essentially, the entire row from the wide-format data-set is deleted/ignored. Researchers that prefer to make use of more sophisticated approaches to missing data, such as multiple imputation, will need to perform this within step 0.

For the purpose of this tutorial, we use the Boot-Err approach to assess in how far random error score variance is identical across administrations of the Conscientiousness dimension from the HEXACO personality inventory. In the Registered Report 10, Verschuere et al. [-@RRRMazar] replicated Experiment 1 from Mazar et al. (2008) across 19 labs. The original study tested the self-concept maintenance theory, which implies that people are willing to be dishonest about their performance in a task if it helps to maximize profits nut only as long as they can hold a positive view of themselves. In the original Experiment 1, participants were asked to solve a number of matrices in a problem-solving task. During the experiment, participants were briefly left alone, allowing them to claim they solved any arbitrary number of matrices. As participants were paid according to their performance in the task, both an opportunity and the incentive to cheat were induced. However, participants were randomly assigned to either a control group, where they recalled 10 books they read in high school, or a treatment group, where they recalled the Ten Commandments. Recalling the Ten Commandments was supposed to serve as a moral reminder, increasing the participants' attention to a moral standard of honesty. While recalling 10 books was a neutral task, recalling the Ten Commandments was supposed to remind participants that cheating for personal gain is "bad" and clashes with a positive self-image. Therefore, Mazar et al. expected participants in the treatment condition to report a lower number of solved matrices than in the control condition. In the original study, the HEXACO personality inventory was included as a filler task and for exploratory analyses regarding potential moderators of dishonesty. We chose the conscientiousness dimension of HEXACO for no specific reason, any of the other five subscales could have been used for the means of this tutorial.

```{r, output = F, warning = F}
# relevant libraries required for this script
packages <- c("magrittr", "dplyr", "osfr", "here", "zip", "data.table", "metafor", "boot")

# check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  pkg_avail <- nzchar(system.file(package = x))   # check if library is installed on system
  
  if(pkg_avail){
    require(x, character.only = TRUE)             # load the library, if already installed
    
  }else{
    install.packages(x)                           # install the library, if missing
    require(x, character.only = TRUE)             # load after installation
  }
})
```

```{r, warning = F, eval = F}
library(here)

dir.create(here("Data"), showWarnings = FALSE)
dir.create(here("Data/Downloaded Data"), showWarnings = FALSE)


# use osfr-package to download RRR10 data on the Mazar et al. / Srull et al. replications
osfr::osf_retrieve_file("https://osf.io/fwnc2") %>% 
  osfr::osf_download(path = here("Data/Downloaded Data/"), conflicts = "overwrite")
# zip-file is stored at Downloaded Data

# again, unzip the file and move it to the corresponding directory
unzip(here("Data/Downloaded Data/Meta-Analysis_2018-07-09.zip"),
      files = "Meta-Analysis/Results/raw_data_corrected_MAA.csv",
      exdir = here("Data/Original Data/RRR10"),
      junkpaths = T)
```

While the data cleaning and manipulation procedure is bound to be different for every research project, we briefly discuss how we come to a final data-set for this tutorial. Firstly, we download the zipped replication data-set from the RRR10 OSF-repository (osf.io/fwnc2), using the osfr R package version 0.2.9 (ref.). After unzipping, we can read the data-file in R. We want to restrict our analysis to only those participants, which were included in the replicators' primary analyses. Therein, we end up with 4,674 participants from 19 laboratories. We achieve this in substep *a)* by identifying the relevant laboratories from the published manuscript and filtering out all laboratories that do not belong to the primary replication.

```{r, warning = F}
# HEXACO Conscientiousness

# a) Load data and select laboratories part of primary replication

# Thankfully, for the RRR10 project, the data can be taken from a file directly, without the need of combination:
pc_df <- as.data.frame(fread(here("Data/Original Data/RRR10/raw_data_corrected_MAA.csv")))

# data <- data[which(data$age >= 18 & data$age <= 25),]
# retain observations only for participants eligible for analysis
pc_df <- pc_df[which(pc_df$inclusion == "inclusion both RRR" | pc_df$inclusion == "inclusion Mazar only"),] %>%
  mutate(source = lab.name)

# identify relevant labs for analsysis
labs_in_paper <- c("Laine", "klein Selle & Rozmann", "Aczel", "Ferreira-Santos", "Meijer", "Loschelder", "Wick", "Suchotzki", 
                   "Sutan", "Vanpaemel", "Verschuere", "Wiggins", "Gonzalez-Iraizoz", "Koppel", "Birt", "McCarthy", "Evans", 
                   "Holzmeister", "Ozdogru")
labs_in_data <- unique(pc_df$source)
labs_in_data[8] <- "Gonzalez-Iraizoz"
labs_in_data[16] <- "Ozdogru"

# remove labs from data, which we do not need for analysis
labs_excl <- labs_in_data[!labs_in_data %in% labs_in_paper]
pc_df <- pc_df[which(!pc_df$source %in% labs_excl),]

# # include only participants in cheat condition (design was 2x2, cheat - no cheat x commandment - books)
# pc_df <- pc_df[which(pc_df$maz.cheat.cond == "cheat"),]

```

The data-set contains a large number of information, that we do not need. Therefore, in substep *b)* we retain only those columns from the data-set, that contain information about the participants' responses to the HEXACO-items and in which laboratory the participant was assessed. Since some of the HEXACO-items are reverse-coded, we need to re-code half of the items, so the scale is identical across all items.

```{r, warning = F}
# b) Select relevant variables and re-code items that are coded in the opposite direction

# retain only those columns, which are needed for subsequent analysis.
pc_df <- pc_df[,c(which(names(pc_df) %in% c("source")),
                  grep("^hex", names(pc_df)))]

# recoding the hexaco items, that need recoding

for(i in grep("^hex", names(pc_df))){
  pc_df[,i] <- as.integer(pc_df[,i])
}
# these are the numbers of the items, that need recoding
items_hex_recode <- c(30, 12, 60, 42, 24, 28, 53, 35, 41, 59, 28, 52, 10, 46, 9, 15, 57, 21, 26, 32, 14, 20, 44, 56, 1, 31, 49, 19, 55, 48)
names_items_hex_recode <- paste0("hex", items_hex_recode) # pasting "hex" and number gives the column names

names_items_hex_recode_R <- paste0(names_items_hex_recode, "_R") # adding _R for names of items, that are recoded
pc_df[,names_items_hex_recode_R] <- 6 - pc_df[,names_items_hex_recode] # recode items that need recoding
```

Lastly, in substep *c)*, we remove all columns that contain responses on the other HEXACO-dimensions, which are not Conscientiousness. Therein, we have constructed a data-set, containing the responses on Conscientiousness from all participants that are part of the primary replication sample in RRR10.

```{r, warning = FALSE}

# c) select items belonging to Conscientiousness subscale

#Conscientiousness
items_hex_CO <- c(2, 26, 8, 32, 14, 38, 50, 20, 44, 56) # items in Conscientiousness subscale
names_items_hex_CO <- ifelse(items_hex_CO %in% items_hex_recode, paste0("hex", items_hex_CO, "_R"), paste0("hex",items_hex_CO)) # did item need recoding?
pc_hex_items_CO <- which(names(pc_df) %in% names_items_hex_CO) # select all items from Conscientiousness subscale, correctly coded

pc_df <- pc_df[,c(names_items_hex_CO, "source")]

# pc_df <- na.omit(pc_df)

labs <- unique(pc_df$source)

```

However, we find that some participants' responses on the Conscientiousness-items are missing in this data-set. While this was of no concern to the replication-authors, as the HEXACO-inventory served as a filler instrument, the same is not true for this tutorial. We will use Cronbach's Alpha to estimate score reliability. Cronbach's Alpha can not deal with missing responses. To keep it simple for this tutorial, we make use of list-wise deletion to remove any missing responses from the data-set. This means that the data for each participant, where any response on one or several items is missing is deleted from the data-set. Alternative approaches, that attempt to impute the missing responses while retaining the general covariance-structure of the data-set are available. Great examples include Multiple Imputation or Multivariate Imputation by Chained Equations. Singe imputation methods like Mean or Mode Imputation restrict the data-sets covariance structure, leading to estimation issues and biases in subsequent analyses.

```{r, warning = FALSE}
# d) dealing with missing responses

colSums(is.na(pc_df))

pc_df <- na.omit(pc_df)
```

Finishing step 0, we have constructed a data-set, containing only the responses of participants that belong to the replication authors' primary analysis sample, additionally removing missing responses from the data-set using listwise deletion. Subsequently, we can begin to follow the Boot-Err procedure.

### Step 1 - estimating score reliability

The first step in the Boot-Err procedure is to generate estimates of score reliability for each sample or administration site respectively. In the following code, we use an *sapply*-loop to compute Cronbach's Alpha separately for each laboratory in the data-set. Cronbach's Alpha is generally defined in the following @eq-alpha_formal (ref.).

$$\alpha=\frac{k^2\ \bar{\sigma}_{ij}}{\sigma^2_x}$$ {#eq-alpha_formal}

Here, $k$ corresponds to the number of items in the scale, $\bar{\sigma}_{ij}$ constitutes the average covariance between items and $\sigma^2_x$ constitutes the total score variance in the total test results.

However, computing the average item covariance can be computationally intensive, and a more efficient computation technique is available in @eq-alpha_fast (ref.).

$$\alpha = \frac{k}{k-1}\left(1-\frac{\sum_{i=1}^k\sigma^2_{i}}{\sum_{i=1}^k\sum_{j=1}^k\ \sigma_{ij}}\right) $$ {#eq-alpha_fast}

Here, $\sigma^2_{i}$ describes the variance of scores on item $i$, while $\sigma_{ij}$ describes the covariance between items $i$ and $j$. The equation in @eq-alpha_fast computes the same Cronbach's Alpha as in @eq-alpha_formal, but much more efficiently. By using *sapply* to quickly loop across all laboratories, we can efficiently generate a vector containing all estimates of Cronbach's Alpha for each administration site respectively.

```{r, warning = F}
alpha_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  C <- cov(dat)
  j <- dim(C)[1]
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  return(alpha)
  
})

alpha_vec
```

### Step 2 - estimating error score variance

After generating estimates of score reliability using Cronbach's Alpha, we can turn towards Step 2 of the Boot-Err procedure - estimating error score variance. According to CTT, error score variance is defined as described in @eq-error_var.

$$ {\sigma}^2_e = \left(1 - \rho_{xx'} \right) \sigma^2_x$$ {#eq-error_var}

This means, provided we generate an estimate of total score variance, we can use @eq-error_est to estimate error score variance, as we already have estimate of score reliability in the form of Cronbach's Alpha readily available.

$$ \hat{\sigma}^2_e = \left(1 - \hat{\alpha} \right) \hat{\sigma}^2_x$$ {#eq-error_est}

Here, $\hat{\alpha}$ describes the estimate of Cronbach's Alpha and $\hat{\sigma}^2_x$ constitutes an estimate of total score variance.

In the following code we, again, use an *sapply*-function to efficiently repeat the same computation for each laboratory. We quickly estimate the total score variance $\sigma^2_x$ by averaging the item scores separately for each participant (generating the test score) and computing the variance of these test scores. Since the *sapply*-function also returns a vector of $\sigma^2_x$-estimates, we can easily use @eq-error_est to generate a vector containing error score variance $\sigma^2_e$-estimates for each laboratory respectively.

```{r}

var_X_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  var_X <- dat %>% 
    rowMeans(.) %>% 
    var(.)
  
  return(var_X)
  
})

var_E_vec <-  (1-alpha_vec) * var_X_vec

var_E_vec

```

### Step 3 - construct Standard Errors using Bootstrapping

As described in the section *Analysis of error score variance - Boot-Err*, we use a random-effects meta-analysis to estimate and test for heterogeneity in error score variances. Since estimation of total score variance and its components is not variance-stable, we need to use a variance-stabilizing transformation. Bartlett and Kendall [-@bartlett1946] show that using the natural log, as in @eq-stable_var, is a suitable variance-stabilizing transformation for total score variance estimates. $$ T[\hat{\sigma}^2_x] = ln \left(\hat{\sigma}^2_x \right) $$ {#eq-stable_var}

While little evidence on variance-stabilizing transformations for score variance components is available, we have used the natural log for both the total and the error score variance component in our proposal for the Boot-Err [@beinhauer2025]. In the simulation scheme, we have not found a large bias in the estimates which would suggest that the transformation described in @eq-stable_var would be unsuitable for the variance component of error score variance $\sigma^2_e$. Accordingly, we are confident in our claim, that @eq-stable_varE describes a suitable variance-stabilizing transformation.

$$ T[\hat{\sigma}^2_e] = ln \left(\hat{\sigma}^2_e \right) $$ {#eq-stable_varE}

However, in order to run a random-effects meta-analysis, we not only need a variance-stable estimate, but also a Standard Error, quantifying our uncertainty in that estimate. While no analytic approximation of a Standard Error for log-transformed error score variance has been derived, we can use Bootstrapping techniques to generate robust Standard Error estimates for $T[\hat{\sigma}^2_e]$.

The package *boot* provides easy-to-use and efficient functions (including parallelization if desired) for Bootstrapping techniques [@boot]. In this tutorial, we make use of version 1.3-31. In order to use *boot*, we need to provide the package with a user-written function, which needs to do two things: i) allow the *boot*-package to sample from our data-set (we do so by `dat_boot <- data[indices,]`), and ii) compute the parameter on the re-sampled data-set that we want to boot-strap. In the following code-chunk, we show a function that does just that.

```{r}

ln_var_E_boot_func <- function(data, indices){
  
  dat_boot <- data[indices,]
  
  C <- cov(dat_boot)
  j <- dim(C)[1]
  alpha <- (1 - sum(diag(C))/sum(C)) * (j/(j - 1))
  
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  var_E <- var_X * (1 - alpha)
  
  ln_var_E <- log(var_E)
  
  return(ln_var_E)
  
}
```

Now that we have a function available for *boot* to re-sample our data and estimate $T[\hat{\sigma}^2_e]$ in the bootstrapped samples, we need to supply our user-defined function to *boot* and compute the Standard Error from the bootstrapped estimates. Essentially, what we do here is estimate the Standard Error of log-transformed error score variance, by computing the standard deviation of all bootstrapped estimates for a single laboratory, see @eq-bootSE.

$$ SE\left[T[\hat{\sigma}^2_e]\right] = \sqrt{\frac{1}{n}\sum_{i=1}^n(T[\hat{\sigma}^2_e]_i - \bar{T}[\hat{\sigma}^2_e])^2}$$ {#eq-bootSE}

Here, $n$ describes the number of bootstrap-samples drawn, whereas $\bar{T}[\hat{\sigma}^2_e]$ describes the mean of the log-transformed error score variance estimates of the $n$ bootstrap samples. In the following code-chunk, we use the *boot* package to estimate $SE\left[T[\hat{\sigma}^2_e]\right]$ from 3000 bootstrap samples for each laboratory respectively. Again, we make use of an *sapply*-function to loop efficiently across laboratories.

```{r, warning = FALSE}


ln_var_E_SE_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_E_boot_func,
                               R = 3000)
  
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})
```

Lastly, we still need to formally transform and store the individual estimates of $T[\hat{\sigma}^2_e]$ for each laboratory (so far we only did this for the bootstrap-samples). However, at this point, computing this transformation is trivial. To make things easier for us in the following steps, we combine the estimates of log-transformed error score variance and its Standard Errors here in a single data-frame. We add the laboratory names to the data-frame, so estimates can be linked to their respective sources.

```{r, warning = FALSE}
ln_var_E_vec <- log(var_E_vec)

ln_var_E_df <- data.frame(ln_var_E = ln_var_E_vec, 
                          ln_var_E_SE = ln_var_E_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

ln_var_E_df
```

Concluding step 3, we have gathered all estimates we need, to perform a random-effects meta-analysis on (log-transformed) error score variances.

### Step 4 - meta-analysis of error score variance

For readers who only wish to use Boot-Err, Step 4 is very simple \[Ich habe mal drüber nachgedacht hier als zusätzlichen input das random-effects model zu erklären - der Part hier ist aber doch etwas länger geworden, braucht's das in dem Sinne denn überhaupt?\]. We use the R-package *metafor*, as it provides an efficient and easy-to-use implementation of random-effects meta-analyses for any parameter [@metafor]. Using the *rma*-function, with the argument *method = "REML"*, ensures that we use the REML-estimator of heterogeneity. Hönekopp and Linden [-@hoenekopp2022] have shown that the REML-estimator tends to outperform other estimators across a wide range of conditions. Therefore, the following chunk will run a random-effects meta-analysis in *metafor*, generating an estimate of the meta-analytic mean (in the output `estimate`), estimates of absolute heterogeneity in standard deviation and variance (in the output `tau` and `tau^2`) and estimates of relative heterogeneity in the form of $I^2$ and $H^2$ (in the output `I^2` and `H^2`). Additionally, *metafor* provides the results for two hypothesis tests. Firstly, the test for heterogeneity (`Test for Heterogeneity` in the output), assessing whether the estimate of heteorgeneity $\tau^2$ can be statistically distinguished from $0$. Secondly, the test for the meta-analytic effect size, assessing whether the meta-analytic aggregate can be statistically distinguished from $0$ (in the Output `Model results`).

```{r}

ln_var_E_rma <- metafor::rma(data = ln_var_E_df,
                             yi = ln_var_E,
                             sei = ln_var_E_SE,
                             method = "REML")

ln_var_E_rma

```

The significance tests indicate two things for the log-transformed error score variance of Conscientiousness across replications in Verschuere et al. [-@RRRMazar]: Firstly, we find that the meta-analytic estimate of `r paste0("$\\mu_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$b[1], 3), "$")` is significantly different from zero (`r paste0("$z = ", round(ln_var_E_rma$zval, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$pval < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$pval, 3))), "$")`), using a significance level of 5%. However, this is not meaningful information, as the estimate is on the log-transformed scale and can not be interpreted. Secondly and more interestingly, we find that the test for heterogeneity indicates that there is statistically significant heterogeneity in log-transformed error score variances (`r paste0("$Q(", ln_var_E_rma$k - 1, ") = ", round(ln_var_E_rma$QE, 3), "$")`, `r paste0("$p ", ifelse(ln_var_E_rma$QEp < .001, yes = paste0('< .001'), no = paste0('= ', round(ln_var_E_rma$QEp, 3))), "$")`). Even though the tested estimates are still on the log-scale, we can nonetheless infer from this result, that error score variance $\sigma^2_e$ is not identical across all administrations of the Conscientiousness-scale. However, the estimate of `r paste0("$\\tau^2_{T[\\hat{\\sigma}^2_e]} = ", round(ln_var_E_rma$tau2, 3), "$")` is uninterpretable due to the log-transformation.

The estimates of relative heterogeneity can be interpreted despite the transformation. The estimate of `r paste0("$I^2 = ", round(ln_var_E_rma$I2, 2), "$")` implies that `r round(ln_var_E_rma$I2, 2)`% of the variance observed in the estimate of error score variance are due to actual heterogeneity in the population, not due to sampling error. Similarly, the estimate of `r paste0("$H^2 = ", round(ln_var_E_rma$H2, 2), "$")` implies that the heterogeneity is `r round(ln_var_E_rma$H2, 2)` times as large as the variance induced by sampling error. According to standard conventions (ref.), both estimate imply a strong or large extent of heterogeneity. However, both estimates of relative heterogeneity are often criticized, as they depend strongly on sample size and are easily manipulated (refs.). Instead, we should turn towards an informative interpretation of the absolute heterogeneity in terms of $\tau^2$ or $\tau$.

To do so, we need to back-transform our newly generated estimates to the original scale. This is done in Step 5.

### Step 5 - Backtransforming estimates

The transformation of score variance (components) using the natural log not only serves to generate variance-stable estimates. Additionally, it comes with the added benefit, that we can assume $T[\sigma^2_e]$ to follow a normal distribution. Several authors have proposed that the total score variance, if not constant, can be described by a log-normal distribution (refs.). This means that the log-transformed estimates accordingly follow a normal distribution. This is very neat for the application of the Boot-Err technique, as one central assumption of a random-effects meta-analysis is that the parameter follows a normal distribution. Additionally, it helps us back-transform the estimates of meta-analytic mean $\mu_{T[\sigma^2_e]}$ and its heterogeneity $\tau^2_{T[\sigma^2_e]}$, as equations are readily available (refs.).

$$ \hat{\mu}_{\sigma^2_e} = \exp\left(\hat{\mu}_{T[\sigma^2_e]} + \frac{1}{2} \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_mu}

@eq-backtransf_mu demonstrates how the mean error score variance $\hat{\mu}_{\sigma^2_e}$ can be estimated from the meta-analytic estimates of $T[\sigma^2_e]$. Here, $\hat{\mu}_{T[\sigma^2_e]}$ represents the estimate of meta-analytic mean of log-transformed error score variance and $\hat{\tau}^2_{T[\sigma^2_e]}$ describes the estimate of its heterogeneity. Similarly, @eq-backtransf_var can be used to derive the heterogeneity of error score variance $\hat{\tau}^2_{\sigma^2_e}$ on its original scale, from the same meta-analytic estimates of \$T\[\sigma\^2_e\]\$\$.

$$ \hat{\tau}^2_{\sigma^2_e} = \exp\left(\hat{\tau}^2_{T[\sigma^2_e]} - 1\right) \cdot \exp\left(2 \hat{\mu}_{T[\sigma^2_e]} + \hat{\tau}^2_{T[\sigma^2_e]}\right) $$ {#eq-backtransf_var}

In the following chunk, we define two functions which compute [Equations -@eq-backtransf_mu] and [-@eq-backtransf_var] from the *metafor*-output. The functions only take the *metafor*-object generated from the *rma*-function as input and return the estimates $\hat{\mu}_{\sigma^2_e}$ and $\hat{\tau}^2_{\sigma^2_e}$ respectively.

```{r}
backtransform_var_mu <- function(rma_obj){
  exp(rma_obj$b[1] + (.5*rma_obj$tau2))
}

backtransform_var_tau2 <- function(rma_obj){
  (exp(rma_obj$tau2) - 1) * exp((2 * rma_obj$b[1]) + rma_obj$tau2)
}

backtransform_var_mu(ln_var_E_rma)

backtransform_var_tau2(ln_var_E_rma) %>% 
  sqrt(.)
```

Running the two functions in the chunk above returns two estimates: The meta-analytic mean error score variance of the HEXACO-Conscientiousness scale across 19 laboratories is $\hat{\mu}_{\sigma^2_e} = `{r} round(backtransform_var_mu(ln_var_E_rma), 3)`$; The estimate for its heterogeneity is $\hat{\tau}_{\sigma^2_e} =`{r} round((backtransform_var_tau2(ln_var_E_rma) %>%   sqrt(.)), 3)`$. To understand what this implies, we can assess the relative size of heterogeneity, also known as the coefficient of variation, which is computed as $CV = \frac{\tau}{\mu}$. Here we find a CV of $CV_{\sigma^2_e} = `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3)`$. This means that the extent of the differences in error score variances is approximately `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance found across these laboratories.

None of the other approaches, such as RG-MA, IRT or measurement invariance testing allow to generate these estimates we have derived using the Boot-Err technique. For researchers who are more accustomed to these scales, these estimates can be highly valuable to understand how much variance in scores we can expect to be introduced by measurement imprecision across administrations of the same scale. Since we are no experts in use of the Conscientiousness-scale, we have to look slightly further beyond. However, as described in the following Bonus-Step, we can follow the Boot-Err technique with an additional parameter, generating much more informative insights.

### Bonus-Step - Relating error score variance to total score variance

Score reliability $\rho_{xx'}$ is defined as the relative extent to which the score variance observed is attributable to actual differences in the true scores, not to measurement imprecision. However, that also means that $1 - \rho_{xx'}$ describes the relative extent to which the score variance is attributable to random measurement error.

Similarly, on the meta-analytic scale, we can look at the heterogeneity in total score variance $\sigma^2_x$, and attempt to understand how much of its heterogeneity came to be through differences in error score variance. Intuitively, many researchers tend to ascribe heterogeneity in total score variance $\sigma^2_x$ to differences in measurement precision. This means that often, a larger score variance is interpreted as if it came to be through larger measurement error such as $\sigma^2_e$. However, obviously parts of the heterogeneity might also be better explained by differences in the true score variance $\sigma^2_t$. Fortunately, if we follow Steps 3 through 5 for the total score variance $\sigma^2_x$ instead, we can assess in how far its heterogeneity can be attributed to differences in true score variance $\sigma^2_t$ or measurement error $\sigma^2_e$.

In the following chunk, we briefly run through these steps to estimate heterogeneity in total score variance $\sigma^2_x$.

```{r}
ln_var_X_boot_func <- function(data, indices){
  
  dat_boot <- data[indices,]
  
  var_X <- dat_boot %>% 
    rowMeans(.) %>% 
    var(.)
  
  ln_var_X <- log(var_X)
  
  return(ln_var_X)
}


ln_var_X_SE_vec <- sapply(labs, FUN = function(lab){
  
  dat <- pc_df %>% 
    filter(source == lab) %>% 
    select(-"source")
  
  boot_estimates <- boot::boot(dat, 
                               statistic = ln_var_X_boot_func,
                               R = 3000)
  
  SE <- sd(boot_estimates$t)
  
  return(SE)
  
})

ln_var_X_vec <- log(var_X_vec)

ln_var_X_df <- data.frame(ln_var_X = ln_var_X_vec, 
                          ln_var_X_SE = ln_var_X_SE_vec, 
                          laboratory = labs, 
                          row.names = NULL)

ln_var_X_rma <- metafor::rma(data = ln_var_X_df,
                             yi = ln_var_X,
                             sei = ln_var_X_SE,
                             method = "REML")


backtransform_var_mu(ln_var_X_rma)

backtransform_var_tau2(ln_var_X_rma) %>% 
  sqrt(.)

```

We find a meta-analytic mean of total score variance $\hat{\mu}_{\sigma^2_x} = `{r} round(backtransform_var_mu(ln_var_X_rma), 3)`$ and a heterogeneity of $\hat{\tau}_{\sigma^2_x} = `{r} round((backtransform_var_tau2(ln_var_X_rma) %>% sqrt(.)), 3)`$. Under CTT, total score variance is defined as the sum of true and error score variance $\sigma^2_x = \sigma^2_e + \sigma^2_t$, which are assumed to be independent. This means that we can assess how much heterogeneity in total score variance can actually be explained by differences in measurement error:

$$ R = \frac{\hat{\tau}^2_{\sigma^2_e}}{\hat{\tau}^2_{\sigma^2_x}} $$ {#eq-ratio}

In @eq-ratio, we define the ratio-variable $R$ as the ratio of $\hat{\tau}_{\sigma^2_e}$ to $\hat{\tau}_{\sigma^2_e}$. For Conscientiousness, we find a ratio of $R  = `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3)`$. This means that about `{r} round((backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma)), 3) * 100`% of heterogeneity in $\sigma^2_x$ could be explained by differences in random measurement error. This also means that `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`% of this heterogeneity needs to be explained by differences in true score variance $\sigma^2_t$.

### Conclusion

Using the Boot-Err technique, we can use random-effects meta-analysis and appropriate variance-stabilizing transformations paired with Bootstrapping techniques to assess in how far there is heterogeneity in random measurement error across administrations of the same scale. Making use of the data supplied by Verschuere et al. [-@RRRMazar], we used Boot-Err to understand in how far there are differences in absolute measurement precision across 19 laboratories where the HEXACO-Conscientiousness personality scale was administered. We not only find statistically significant differences in error score variance, but were also able to estimate the absolute and relative extent of these differences. Therein, we are certain that the random measurement error in the Conscientiousness scores is not constant across samples. We find that the heterogeneity is about `{r} round((backtransform_var_tau2(ln_var_E_rma) %>% sqrt(.))/backtransform_var_mu(ln_var_E_rma), 3) * 100`% of the mean error score variance. Across administrations of the same scale, error score variance easily varies between `r round(exp(ln_var_E_rma$b[1] - sqrt(ln_var_E_rma$tau2)), 3)` and `r round(exp(ln_var_E_rma$b[1] + sqrt(ln_var_E_rma$tau2)), 3)` (mean value plus/minus heterogeneity, then back-transformed) \[add this to Bonus-Step?\].

In order to better understand what these differences mean, we additionally uncovered in how far the heterogeneity in total score variance can be attributed to these differences in measurement error. Here, we found that the overwhelming majority of heterogeneity, `{r} round((1 - (backtransform_var_tau2(ln_var_E_rma)) / (backtransform_var_tau2(ln_var_X_rma))), 3) * 100`%, can not be explained by differences in measurement error. For the Conscientiousness-scale, this implies that, while there clearly are substantial differences in measurement error across groups, the distribution of true scores varies much more strongly across groups. This is important, as this implies that the differences in total score variance can not be attributed to differences in measurement error. Instead, Conscientiousness is distributed highly different across those groups. In Beinhauer et al. [-@beinhauer2025liable] we discuss potential causes and implications of this phenomenon.

## Discussion

Measurement precision and how strongly it diverges across administrations of the same instrument is a central question in social and behavioural sciences. Differences in random measurement error are not only interesting to researchers assessing the quality of a measuring instrument. It also has clear implications when it comes to hypothesis testing and standardization of effect sizes. Therein, the assessment of measurement equivalence has implications for replication studies and meta-analytic applications. Across the first pages, we have summarised the most popular techniques to assess differences in random measurement error - Reliability Generalization Meta-Analysis, Measurement Invariance Testing and approaches stemming from Item Response Theory. Additionally, we introduced Boot-Err, a technique we developed to step away from relative measures such as score reliability or tests without estimates of how large the differences are, like Measurement Invariance Tests [@beinhauer2025].

In this tutorial, we have demonstrated how the Boot-Err can be used to generate detailed estimates and tests regarding the extent of the differences we observe in random measurement error, and how strongly they affect the differences in score variance. It is important to note that this technique comes with a number of limitations and constraints to generalizability. Just like for any statistical analysis, a number of assumptions need to be met. Regarding application of the Boot-Err, total and error score variance should follow a log-normal distribution and be computed from samples that are generally independent of each other (the individual samples should not be described as if stemming from a nested structure). Additionally, a number of assumptions relate to the type of score reliability estimate used. Here, we made use of Cronbach's Alpha, which typically comes with assumptions of tau-equivalence and unidimensionality (ref.). While these assumptions are widely criticized and rarely hold up in real data (ref.), we need to remind ourselves that these assumptions also relate to typical scoring methods of these measuring instrument. The HEXACO-personality score for each dimension is computed by taking the average of the relevant item scores \[check to make sure this is correct\]. Therein, even stricter assumptions of tau-equivalence and unidimensionality apply - making Cronbach's Alpha the more appropriate choice of score reliability estimate over other ones that may impose less strict assumptions.

Even though differences in absolute measurement precision are of central interest to measuring instrument and scale development, methods to assess differences in error score variance each come with their own caveats and shortcomings. Models regarding differences in score reliability fail to take differences introduced by true score variance into account, while models simply testing for differences in error score variance without actually estimating it fail to put these differences into perspective. Perhaps this is the reason why discussions surrounding the impact of random measurement error on standardized effect sizes and hypothesis tests has been overemphasized (refs. \[NEED TO PICK THIS UP IN INTRO\]), implying that removing random measurement error was akin to harmonizing variability across populations and data-sets.

Boot-Err, mends both these shortcomings, as differences in error score variance can be estimated and tested, but also put into perspective on how much heterogeneity in total score variance they actually produce. As we demonstrated in the Bonus-Step section, differences in error score variance can not explain why total score variance differs across groups--instead, differences in true score variance must come at a much larger extent. This implies that differences in score variability can not sufficiently be mended by taking care of measurement error. Instead, future research should ensure that the true score variance is stable across groups and representative for the respective population.

#### Abstellgleis

DAS HIER KÖNNT IHR IGNORIEREN - DAS SIND ALTE TEXTBAUSTEINE DIE FÜR DEN MOMENT KEINEN PLATZ HABEN

One central assumption of IRT is that the item characteristics are valid for all subgroups in the population to which the model is applied. This means that, no matter the sub-group the individual is from, the item-response-function should be identical. This means that all individuals with the same latent ability score $\theta$ have the same probability of getting an item right or wrong. As this is a central assumption of these models, routine procedures for examining differential item functioning (DIF) have been established. Typically, a *reference sample* should be available, where an IRT-scoring model has been established. Subsequently, we can assess if any items exhibit DIF in a *focal group*, where the same $\theta$ would lead to different observed scores.

## References

::: {#refs}
:::
